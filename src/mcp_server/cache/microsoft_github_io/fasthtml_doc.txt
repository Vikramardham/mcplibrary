<project title="microsoft.github.io Link Analysis" summary='A collection of content from microsoft.github.io pages organized by hierarchy.'>

# Table of Contents

- autogen
  - 0.2
  - dotnet
  - stable
    - index.html
    - reference
      - index.html
    - user-guide
      - agentchat-user-guide
        - index.html
        - installation.html
        - migration-guide.html
        - tutorial
          - agents.html
          - human-in-the-loop.html
          - index.html
          - messages.html
          - models.html
          - teams.html
          - termination.html
      - autogenstudio-user-guide
        - index.html
      - core-user-guide
        - index.html
      - extensions-user-guide
        - index.html

## Document Sections

- [autogen](#autogen)
  - [0.2](#autogen_0.2)
  - [dotnet](#autogen_dotnet)
  - [stable](#autogen_stable)
    - [index.html](#autogen_stable_index.html)
    - [reference](#autogen_stable_reference)
      - [index.html](#autogen_stable_reference_index.html)
    - [user-guide](#autogen_stable_user-guide)
      - [agentchat-user-guide](#autogen_stable_user-guide_agentchat-user-guide)
        - [index.html](#autogen_stable_user-guide_agentchat-user-guide_index.html)
        - [installation.html](#autogen_stable_user-guide_agentchat-user-guide_installation.html)
        - [migration-guide.html](#autogen_stable_user-guide_agentchat-user-guide_migration-guide.html)
        - [tutorial](#autogen_stable_user-guide_agentchat-user-guide_tutorial)
          - [agents.html](#autogen_stable_user-guide_agentchat-user-guide_tutorial_agents.html)
          - [human-in-the-loop.html](#autogen_stable_user-guide_agentchat-user-guide_tutorial_human-in-the-loop.html)
          - [index.html](#autogen_stable_user-guide_agentchat-user-guide_tutorial_index.html)
          - [messages.html](#autogen_stable_user-guide_agentchat-user-guide_tutorial_messages.html)
          - [models.html](#autogen_stable_user-guide_agentchat-user-guide_tutorial_models.html)
          - [teams.html](#autogen_stable_user-guide_agentchat-user-guide_tutorial_teams.html)
          - [termination.html](#autogen_stable_user-guide_agentchat-user-guide_tutorial_termination.html)
      - [autogenstudio-user-guide](#autogen_stable_user-guide_autogenstudio-user-guide)
        - [index.html](#autogen_stable_user-guide_autogenstudio-user-guide_index.html)
      - [core-user-guide](#autogen_stable_user-guide_core-user-guide)
        - [index.html](#autogen_stable_user-guide_core-user-guide_index.html)
      - [extensions-user-guide](#autogen_stable_user-guide_extensions-user-guide)
        - [index.html](#autogen_stable_user-guide_extensions-user-guide_index.html)

---

<a id="autogen_stable_index.html"></a>

<doc title="../../index.html" desc="Content from https://microsoft.github.io/autogen/stable/index.html">

---
title: AutoGen
url: https://microsoft.github.io/autogen/stable/index.html
hostname: github.io
description: Top-level documentation for AutoGen, a framework for developing applications using AI agents
sitename: microsoft.github.io
date: 2024-01-01
---
# AutoGen[#](https://microsoft.github.io#autogen)

# AutoGen

### A framework for building AI agents and applications

```
# pip install -U "autogen-agentchat" "autogen-ext[openai]"
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
async def main() -> None:
agent = AssistantAgent("assistant", OpenAIChatCompletionClient(model="gpt-4o"))
print(await agent.run(task="Say 'Hello World!'"))
asyncio.run(main())
```

*Start here if you are building conversational agents. Migrating from AutoGen 0.2?.*

An event-driven programming framework for building scalable multi-agent AI systems. Example scenarios:

Deterministic and dynamic agentic workflows for business processes.

Research on multi-agent collaboration.

Distributed agents for multi-language applications.


*Start here if you are building workflows or distributed agent systems.*

Implementations of Core and AgentChat components that interface with external services or other libraries. You can find and use community extensions or create your own. Examples of built-in extensions:

for using LangChain tools.`LangChainToolAdapter`

for using Assistant API.`OpenAIAssistantAgent`

for running model-generated code in a Docker container.`DockerCommandLineCodeExecutor`

for distributed agents.`GrpcWorkerAgentRuntime`
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_index.html"></a>

<doc title="AgentChat" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html">

---
title: AgentChat#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/index.html
hostname: github.io
description: User Guide for AgentChat, a high-level API for AutoGen
sitename: microsoft.github.io
date: 2024-01-01
---
# AgentChat[#](https://microsoft.github.io#agentchat)

AgentChat is a high-level API for building multi-agent applications.
It is built on top of the [ autogen-core](https://microsoft.github.io/core-user-guide/index.html) package.
For beginner users, AgentChat is the recommended starting point.
For advanced users,

[’s event-driven programming model provides more flexibility and control over the underlying components.](https://microsoft.github.io/core-user-guide/index.html)

`autogen-core`

AgentChat provides intuitive defaults, such as **Agents** with preset
behaviors and **Teams** with predefined [multi-agent design patterns](https://microsoft.github.io/core-user-guide/design-patterns/intro.html).

How to install AgentChat

[./installation.html](https://microsoft.github.io/installation.html)

Build your first agent

[./quickstart.html](https://microsoft.github.io/quickstart.html)

Step-by-step guide to using AgentChat, learn about agents, teams, and more

[./tutorial/index.html](https://microsoft.github.io/tutorial/index.html)

Create your own agents with custom behaviors

[./custom-agents.html](https://microsoft.github.io/custom-agents.html)

Multi-agent coordination through a shared context and centralized, customizable selector

[./selector-group-chat.html](https://microsoft.github.io/selector-group-chat.html)

Multi-agent coordination through a shared context and localized, tool-based selector

[./swarm.html](https://microsoft.github.io/swarm.html)

Get started with Magentic-One

[./magentic-one.html](https://microsoft.github.io/magentic-one.html)

Add memory capabilities to your agents

[./memory.html](https://microsoft.github.io/memory.html)

Log traces and internal messages

[./logging.html](https://microsoft.github.io/logging.html)

Serialize and deserialize components

[./serialize-components.html](https://microsoft.github.io/serialize-components.html)

Sample code and use cases

[./examples/index.html](https://microsoft.github.io/examples/index.html)

How to migrate from AutoGen 0.2.x to 0.4.x.

[./migration-guide.html](https://microsoft.github.io/migration-guide.html)
</doc>

<a id="autogen_stable_user-guide_core-user-guide_index.html"></a>

<doc title="Core" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html">

---
title: Core#
url: https://microsoft.github.io/autogen/stable/user-guide/core-user-guide/index.html
hostname: github.io
description: User Guide for AutoGen Core, a framework for building multi-agent applications with AI agents.
sitename: microsoft.github.io
date: 2024-01-01
---
# Core[#](https://microsoft.github.io#core)

AutoGen core offers an easy way to quickly build event-driven, distributed, scalable, resilient AI agent systems. Agents are developed by using the [Actor model](https://en.wikipedia.org/wiki/Actor_model). You can build and run your agent system locally and easily move to a distributed system in the cloud when you are ready.

Key features of AutoGen core include:

Asynchronous Messaging

Agents communicate through asynchronous messages, enabling event-driven and request/response communication models.

Scalable & Distributed

Enable complex scenarios with networks of agents across organizational boundaries.

Multi-Language Support

Python & Dotnet interoperating agents today, with more languages coming soon.

Modular & Extensible

Highly customizable with features like custom agents, memory as a service, tools registry, and model library.

Observable & Debuggable

Easily trace and debug your agent systems.

Event-Driven Architecture

Build event-driven, distributed, scalable, and resilient AI agent systems.
</doc>

<a id="autogen_stable_user-guide_extensions-user-guide_index.html"></a>

<doc title="Extensions" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html">

---
title: Extensions#
url: https://microsoft.github.io/autogen/stable/user-guide/extensions-user-guide/index.html
hostname: github.io
description: User Guide for AutoGen Extensions, a framework for building multi-agent applications with AI agents.
sitename: microsoft.github.io
date: 2024-01-01
---
# Extensions[#](https://microsoft.github.io#extensions)

AutoGen is designed to be extensible. The `autogen-ext`

package contains the built-in component implementations maintained by the AutoGen project.

Examples of components include:

`autogen_ext.agents.*`

for agent implementations like`MultimodalWebSurfer`

`autogen_ext.models.*`

for model clients likeand`OpenAIChatCompletionClient`

for connecting to hosted and local models.`SKChatCompletionAdapter`

`autogen_ext.tools.*`

for tools like GraphRAGand`LocalSearchTool`

.`mcp_server_tools()`

`autogen_ext.executors.*`

for executors likeand`DockerCommandLineCodeExecutor`

`ACADynamicSessionsCodeExecutor`

`autogen_ext.runtimes.*`

for agent runtimes like`GrpcWorkerAgentRuntime`


See [API Reference](https://microsoft.github.io/reference/index.html) for the full list of components and their APIs.

We strongly encourage developers to build their own components and publish them as part of the ecosytem.

Discover community extensions and samples

[./discover.html](https://microsoft.github.io/discover.html)

Create your own extension

[./create-your-own.html](https://microsoft.github.io/create-your-own.html)
</doc>

<a id="autogen_stable_user-guide_autogenstudio-user-guide_index.html"></a>

<doc title="Studio" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html">

---
title: AutoGen Studio#
url: https://microsoft.github.io/autogen/stable/user-guide/autogenstudio-user-guide/index.html
hostname: github.io
description: User Guide for AutoGen Studio - A low code tool for building and debugging multi-agent systems
sitename: microsoft.github.io
date: 2024-01-01
---
# AutoGen Studio[#](https://microsoft.github.io#autogen-studio)

AutoGen Studio is a low-code interface built to help you rapidly prototype AI agents, enhance them with tools, compose them into teams and interact with them to accomplish tasks. It is built on [AutoGen AgentChat](https://microsoft.github.io/autogen) - a high-level API for building multi-agent applications.

See a video tutorial on AutoGen Studio v0.4 (02/25) -

[https://youtu.be/oum6EI7wohM]

Code for AutoGen Studio is on GitHub at [microsoft/autogen](https://github.com/microsoft/autogen/tree/main/python/packages/autogen-studio)

Caution

AutoGen Studio is meant to help you rapidly prototype multi-agent workflows and demonstrate an example of end user interfaces built with AutoGen. It is not meant to be a production-ready app. Developers are encouraged to use the AutoGen framework to build their own applications, implementing authentication, security and other features required for deployed applications.

## Capabilities - What Can You Do with AutoGen Studio?[#](https://microsoft.github.io#capabilities-what-can-you-do-with-autogen-studio)

AutoGen Studio offers four main interfaces to help you build and manage multi-agent systems:

**Team Builder**A visual interface for creating agent teams through declarative specification (JSON) or drag-and-drop

Supports configuration of all core components: teams, agents, tools, models, and termination conditions

Fully compatible with AgentChat’s component definitions


**Playground**Interactive environment for testing and running agent teams

Features include:

Live message streaming between agents

Visual representation of message flow through a control transition graph

Interactive sessions with teams using UserProxyAgent

Full run control with the ability to pause or stop execution



**Gallery**Central hub for discovering and importing community-created components

Enables easy integration of third-party components


**Deployment**Export and run teams in python code

Setup and test endpoints based on a team configuration

Run teams in a docker container



### Roadmap[#](https://microsoft.github.io#roadmap)

Review project roadmap and issues [here](https://github.com/microsoft/autogen/issues/4006) .

## Contribution Guide[#](https://microsoft.github.io#contribution-guide)

We welcome contributions to AutoGen Studio. We recommend the following general steps to contribute to the project:

Review the overall AutoGen project

[contribution guide](https://github.com/microsoft/autogen/blob/main/CONTRIBUTING.md)Please review the AutoGen Studio

[roadmap](https://github.com/microsoft/autogen/issues/4006)to get a sense of the current priorities for the project. Help is appreciated especially with Studio issues tagged with`help-wanted`

Please use the tag

tag for any issues, questions, and PRs related to Studio`proj-studio`

Please initiate a discussion on the roadmap issue or a new issue to discuss your proposed contribution.

Submit a pull request with your contribution!

If you are modifying AutoGen Studio, it has its own devcontainer. See instructions in

`.devcontainer/README.md`

to use it

## A Note on Security[#](https://microsoft.github.io#a-note-on-security)

AutoGen Studio is a research prototype and is **not meant to be used** in a production environment. Some baseline practices are encouraged e.g., using Docker code execution environment for your agents.

However, other considerations such as rigorous tests related to jailbreaking, ensuring LLMs only have access to the right keys of data given the end user’s permissions, and other security features are not implemented in AutoGen Studio.

If you are building a production application, please use the AutoGen framework and implement the necessary security features.

## Acknowledgements and Citation[#](https://microsoft.github.io#acknowledgements-and-citation)

AutoGen Studio is based on the [AutoGen](https://microsoft.github.io/autogen) project. It was adapted from a research prototype built in October 2023 (original credits: Victor Dibia, Gagan Bansal, Adam Fourney, Piali Choudhury, Saleema Amershi, Ahmed Awadallah, Chi Wang).

If you use AutoGen Studio in your research, please cite the following paper:

```
@inproceedings{autogenstudio,
title={AUTOGEN STUDIO: A No-Code Developer Tool for Building and Debugging Multi-Agent Systems},
author={Dibia, Victor and Chen, Jingya and Bansal, Gagan and Syed, Suff and Fourney, Adam and Zhu, Erkang and Wang, Chi and Amershi, Saleema},
booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
pages={72--79},
year={2024}
}
```

## Next Steps[#](https://microsoft.github.io#next-steps)

To begin, follow the [installation instructions](https://microsoft.github.io/installation.html) to install AutoGen Studio.
</doc>

<a id="autogen_stable_reference_index.html"></a>

<doc title="API Reference" desc="Content from https://microsoft.github.io/autogen/stable/reference/index.html">

---
title: API Reference#
url: https://microsoft.github.io/autogen/stable/reference/index.html
hostname: github.io
description: AutoGen is a community-driven project. Learn how to get involved, contribute, and connect with the community.
sitename: microsoft.github.io
date: 2024-01-01
---
# API Reference[#](https://microsoft.github.io#api-reference)

AutoGen AgentChat

AutoGen Core

[autogen_core](https://microsoft.github.io/python/autogen_core.html)`Agent`

`AgentId`

`AgentProxy`

`AgentMetadata`

`AgentRuntime`

`BaseAgent`

`CacheStore`

`InMemoryStore`

`CancellationToken`

`AgentInstantiationContext`

`TopicId`

`Subscription`

`MessageContext`

`AgentType`

`SubscriptionInstantiationContext`

`MessageHandlerContext`

`MessageSerializer`

`UnknownPayload`

`Image`

`RoutedAgent`

`ClosureAgent`

`ClosureContext`

`message_handler()`

`event()`

`rpc()`

`FunctionCall`

`TypeSubscription`

`DefaultSubscription`

`DefaultTopicId`

`default_subscription()`

`type_subscription()`

`TypePrefixSubscription`

`JSON_DATA_CONTENT_TYPE`

`PROTOBUF_DATA_CONTENT_TYPE`

`SingleThreadedAgentRuntime`

`ROOT_LOGGER_NAME`

`EVENT_LOGGER_NAME`

`TRACE_LOGGER_NAME`

`Component`

`ComponentBase`

`ComponentFromConfig`

`ComponentLoader`

`ComponentModel`

`ComponentSchemaType`

`ComponentToConfig`

`is_component_class()`

`is_component_instance()`

`DropMessage`

`InterventionHandler`

`DefaultInterventionHandler`

`ComponentType`


[autogen_core.code_executor](https://microsoft.github.io/python/autogen_core.code_executor.html)[autogen_core.models](https://microsoft.github.io/python/autogen_core.models.html)[autogen_core.model_context](https://microsoft.github.io/python/autogen_core.model_context.html)[autogen_core.tools](https://microsoft.github.io/python/autogen_core.tools.html)[autogen_core.tool_agent](https://microsoft.github.io/python/autogen_core.tool_agent.html)[autogen_core.memory](https://microsoft.github.io/python/autogen_core.memory.html)[autogen_core.exceptions](https://microsoft.github.io/python/autogen_core.exceptions.html)[autogen_core.logging](https://microsoft.github.io/python/autogen_core.logging.html)

AutoGen Extensions

[autogen_ext.agents.magentic_one](https://microsoft.github.io/python/autogen_ext.agents.magentic_one.html)[autogen_ext.agents.openai](https://microsoft.github.io/python/autogen_ext.agents.openai.html)[autogen_ext.agents.web_surfer](https://microsoft.github.io/python/autogen_ext.agents.web_surfer.html)[autogen_ext.agents.file_surfer](https://microsoft.github.io/python/autogen_ext.agents.file_surfer.html)[autogen_ext.agents.video_surfer](https://microsoft.github.io/python/autogen_ext.agents.video_surfer.html)[autogen_ext.agents.video_surfer.tools](https://microsoft.github.io/python/autogen_ext.agents.video_surfer.tools.html)[autogen_ext.teams.magentic_one](https://microsoft.github.io/python/autogen_ext.teams.magentic_one.html)[autogen_ext.models.cache](https://microsoft.github.io/python/autogen_ext.models.cache.html)[autogen_ext.models.openai](https://microsoft.github.io/python/autogen_ext.models.openai.html)[autogen_ext.models.replay](https://microsoft.github.io/python/autogen_ext.models.replay.html)[autogen_ext.models.azure](https://microsoft.github.io/python/autogen_ext.models.azure.html)[autogen_ext.models.anthropic](https://microsoft.github.io/python/autogen_ext.models.anthropic.html)[autogen_ext.models.semantic_kernel](https://microsoft.github.io/python/autogen_ext.models.semantic_kernel.html)[autogen_ext.models.ollama](https://microsoft.github.io/python/autogen_ext.models.ollama.html)[autogen_ext.models.llama_cpp](https://microsoft.github.io/python/autogen_ext.models.llama_cpp.html)[autogen_ext.tools.code_execution](https://microsoft.github.io/python/autogen_ext.tools.code_execution.html)[autogen_ext.tools.graphrag](https://microsoft.github.io/python/autogen_ext.tools.graphrag.html)[autogen_ext.tools.http](https://microsoft.github.io/python/autogen_ext.tools.http.html)[autogen_ext.tools.langchain](https://microsoft.github.io/python/autogen_ext.tools.langchain.html)[autogen_ext.tools.mcp](https://microsoft.github.io/python/autogen_ext.tools.mcp.html)[autogen_ext.tools.semantic_kernel](https://microsoft.github.io/python/autogen_ext.tools.semantic_kernel.html)[autogen_ext.code_executors.local](https://microsoft.github.io/python/autogen_ext.code_executors.local.html)[autogen_ext.code_executors.docker](https://microsoft.github.io/python/autogen_ext.code_executors.docker.html)[autogen_ext.code_executors.jupyter](https://microsoft.github.io/python/autogen_ext.code_executors.jupyter.html)[autogen_ext.code_executors.azure](https://microsoft.github.io/python/autogen_ext.code_executors.azure.html)[autogen_ext.cache_store.diskcache](https://microsoft.github.io/python/autogen_ext.cache_store.diskcache.html)[autogen_ext.cache_store.redis](https://microsoft.github.io/python/autogen_ext.cache_store.redis.html)[autogen_ext.runtimes.grpc](https://microsoft.github.io/python/autogen_ext.runtimes.grpc.html)[autogen_ext.auth.azure](https://microsoft.github.io/python/autogen_ext.auth.azure.html)[autogen_ext.experimental.task_centric_memory](https://microsoft.github.io/python/autogen_ext.experimental.task_centric_memory.html)[autogen_ext.experimental.task_centric_memory.utils](https://microsoft.github.io/python/autogen_ext.experimental.task_centric_memory.utils.html)
</doc>

<a id="autogen_dotnet"></a>

<doc title=".NET" desc="Content from https://microsoft.github.io/autogen/dotnet">

---
title: Redirecting...
url: https://microsoft.github.io/autogen/dotnet
hostname: github.io
sitename: microsoft.github.io
---
If you are not redirected automatically, follow this
link to example.com
.
If you are not redirected automatically, follow this
link to example.com
.
</doc>

<a id="autogen_0.2"></a>

<doc title="0.2 Docs" desc="Content from https://microsoft.github.io/autogen/0.2">

---
title: AutoGen | AutoGen 0.2
url: https://microsoft.github.io/autogen/0.2/
hostname: github.io
description: Enabling Next-Gen LLM Applications via Multi-Agent Conversation Framework
sitename: AutoGen 0.2
date: 2025-01-01
---
# AutoGen 0.2

An Open-Source Programming Framework for Agentic AI


### Multi-Agent Conversation Framework

AutoGen provides multi-agent conversation framework as a high-level abstraction. With this framework, one can conveniently build LLM workflows.


### Easily Build Diverse Applications

AutoGen offers a collection of working systems spanning a wide range of applications from various domains and complexities.


### Enhanced LLM Inference & Optimization

AutoGen supports enhanced LLM inference APIs, which can be used to improve inference performance and reduce cost.
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_installation.html"></a>

<doc title="Installation" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html">

---
title: Installation#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/installation.html
hostname: github.io
description: Installing AutoGen AgentChat
sitename: microsoft.github.io
date: 2024-01-01
---
# Installation[#](https://microsoft.github.io#installation)

## Create a Virtual Environment (optional)[#](https://microsoft.github.io#create-a-virtual-environment-optional)

When installing AgentChat locally, we recommend using a virtual environment for the installation. This will ensure that the dependencies for AgentChat are isolated from the rest of your system.

Create and activate:

```
# On Windows, change `python3` to `python` (if `python` is Python 3).
python3 -m venv .venv
# On Windows, change `bin` to `scripts`.
source .venv/bin/activate
```

To deactivate later, run:

```
deactivate
```

[Install Conda](https://docs.conda.io/projects/conda/en/stable/user-guide/install/index.html) if you have not already.

Create and activate:

```
conda create -n autogen python=3.12
conda activate autogen
```

To deactivate later, run:

```
conda deactivate
```

## Install Using pip[#](https://microsoft.github.io#install-using-pip)

Install the `autogen-agentchat`

package using pip:

```
pip install -U "autogen-agentchat"
```

Note

Python 3.10 or later is required.

## Install OpenAI for Model Client[#](https://microsoft.github.io#install-openai-for-model-client)

To use the OpenAI and Azure OpenAI models, you need to install the following extensions:

```
pip install "autogen-ext[openai]"
```

If you are using Azure OpenAI with AAD authentication, you need to install the following:

```
pip install "autogen-ext[azure]"
```
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_migration-guide.html"></a>

<doc title="Migration Guide for v0.2 to v0.4" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html">

---
title: Migration Guide for v0.2 to v0.4#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/migration-guide.html
hostname: github.io
sitename: Migration Guide for v0.2 to v0.4
date: 2024-09-01
---
# Migration Guide for v0.2 to v0.4[#](https://microsoft.github.io#migration-guide-for-v0-2-to-v0-4)

This is a migration guide for users of the `v0.2.*`

versions of `autogen-agentchat`

to the `v0.4`

version, which introduces a new set of APIs and features.
The `v0.4`

version contains breaking changes. Please read this guide carefully.
We still maintain the `v0.2`

version in the `0.2`

branch; however,
we highly recommend you upgrade to the `v0.4`

version.

Note

We no longer have admin access to the `pyautogen`

PyPI package, and
the releases from that package are no longer from Microsoft since version 0.2.34.
To continue use the `v0.2`

version of AutoGen, install it using `autogen-agentchat~=0.2`

.
Please read our [clarification statement](https://github.com/microsoft/autogen/discussions/4217) regarding forks.

## What is `v0.4`

?[#](https://microsoft.github.io#what-is-v0-4)

Since the release of AutoGen in 2023, we have intensively listened to our community and users from small startups and large enterprises, gathering much feedback. Based on that feedback, we built AutoGen `v0.4`

, a from-the-ground-up rewrite adopting an asynchronous, event-driven architecture to address issues such as observability, flexibility, interactive control, and scale.

The `v0.4`

API is layered:
the [Core API](https://microsoft.github.io/core-user-guide/index.html) is the foundation layer offering a scalable, event-driven actor framework for creating agentic workflows;
the [AgentChat API](https://microsoft.github.io/index.html) is built on Core, offering a task-driven, high-level framework for building interactive agentic applications. It is a replacement for AutoGen `v0.2`

.

Most of this guide focuses on `v0.4`

’s AgentChat API; however, you can also build your own high-level framework using just the Core API.

## New to AutoGen?[#](https://microsoft.github.io#new-to-autogen)

Jump straight to the [AgentChat Tutorial](https://microsoft.github.io/tutorial/models.html) to get started with `v0.4`

.

## What’s in this guide?[#](https://microsoft.github.io#what-s-in-this-guide)

We provide a detailed guide on how to migrate your existing codebase from `v0.2`

to `v0.4`

.

See each feature below for detailed information on how to migrate.

The following features currently in `v0.2`

will be provided in the future releases of `v0.4.*`

versions:

Model Client Cost

[#4835](https://github.com/microsoft/autogen/issues/4835)Teachable Agent

RAG Agent


We will update this guide when the missing features become available.

## Model Client[#](https://microsoft.github.io#model-client)

In `v0.2`

you configure the model client as follows, and create the `OpenAIWrapper`

object.

```
from autogen.oai import OpenAIWrapper
config_list = [
{"model": "gpt-4o", "api_key": "sk-xxx"},
{"model": "gpt-4o-mini", "api_key": "sk-xxx"},
]
model_client = OpenAIWrapper(config_list=config_list)
```


Note: In AutoGen 0.2, the OpenAI client would try configs in the list until one worked. 0.4 instead expects a specfic model configuration to be chosen.

In `v0.4`

, we offer two ways to create a model client.

### Use component config[#](https://microsoft.github.io#use-component-config)

AutoGen 0.4 has a [generic component configuration system](https://microsoft.github.io/core-user-guide/framework/component-config.html). Model clients are a great use case for this. See below for how to create an OpenAI chat completion client.

```
from autogen_core.models import ChatCompletionClient
config = {
"provider": "OpenAIChatCompletionClient",
"config": {
"model": "gpt-4o",
"api_key": "sk-xxx" # os.environ["...']
}
}
model_client = ChatCompletionClient.load_component(config)
```

### Use model client class directly[#](https://microsoft.github.io#use-model-client-class-directly)

Open AI:

```
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="sk-xxx")
```

Azure OpenAI:

```
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
model_client = AzureOpenAIChatCompletionClient(
azure_deployment="gpt-4o",
azure_endpoint="https://<your-endpoint>.openai.azure.com/",
model="gpt-4o",
api_version="2024-09-01-preview",
api_key="sk-xxx",
)
```

Read more on [ OpenAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient).

## Model Client for OpenAI-Compatible APIs[#](https://microsoft.github.io#model-client-for-openai-compatible-apis)

You can use a the `OpenAIChatCompletionClient`

to connect to an OpenAI-Compatible API,
but you need to specify the `base_url`

and `model_info`

.

```
from autogen_ext.models.openai import OpenAIChatCompletionClient
custom_model_client = OpenAIChatCompletionClient(
model="custom-model-name",
base_url="https://custom-model.com/reset/of/the/path",
api_key="placeholder",
model_info={
"vision": True,
"function_calling": True,
"json_output": True,
"family": "unknown",
},
)
```


Note: We don’t test all the OpenAI-Compatible APIs, and many of them works differently from the OpenAI API even though they may claim to suppor it. Please test them before using them.

Read about [Model Clients](https://microsoft.github.io/tutorial/models.html)
in AgentChat Tutorial and more detailed information on [Core API Docs](https://microsoft.github.io/core-user-guide/components/model-clients.html)

Support for other hosted models will be added in the future.

## Model Client Cache[#](https://microsoft.github.io#model-client-cache)

In `v0.2`

, you can set the cache seed through the `cache_seed`

parameter in the LLM config.
The cache is enabled by default.

```
llm_config = {
"config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
"seed": 42,
"temperature": 0,
"cache_seed": 42,
}
```

In `v0.4`

, the cache is not enabled by default, to use it you need to use a
[ ChatCompletionCache](https://microsoft.github.io/reference/python/autogen_ext.models.cache.html#autogen_ext.models.cache.ChatCompletionCache) wrapper around the model client.

You can use a [ DiskCacheStore](https://microsoft.github.io/reference/python/autogen_ext.cache_store.diskcache.html#autogen_ext.cache_store.diskcache.DiskCacheStore) or

[to store the cache.](https://microsoft.github.io/reference/python/autogen_ext.cache_store.redis.html#autogen_ext.cache_store.redis.RedisStore)

`RedisStore`

```
pip install -U "autogen-ext[openai, diskcache, redis]"
```

Here’s an example of using `diskcache`

for local caching:

```
import asyncio
import tempfile
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.models.cache import ChatCompletionCache, CHAT_CACHE_VALUE_TYPE
from autogen_ext.cache_store.diskcache import DiskCacheStore
from diskcache import Cache
async def main():
with tempfile.TemporaryDirectory() as tmpdirname:
# Initialize the original client
openai_model_client = OpenAIChatCompletionClient(model="gpt-4o")
# Then initialize the CacheStore, in this case with diskcache.Cache.
# You can also use redis like:
# from autogen_ext.cache_store.redis import RedisStore
# import redis
# redis_instance = redis.Redis()
# cache_store = RedisCacheStore[CHAT_CACHE_VALUE_TYPE](redis_instance)
cache_store = DiskCacheStore[CHAT_CACHE_VALUE_TYPE](Cache(tmpdirname))
cache_client = ChatCompletionCache(openai_model_client, cache_store)
response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
print(response) # Should print response from OpenAI
response = await cache_client.create([UserMessage(content="Hello, how are you?", source="user")])
print(response) # Should print cached response
asyncio.run(main())
```

## Assistant Agent[#](https://microsoft.github.io#assistant-agent)

In `v0.2`

, you create an assistant agent as follows:

```
from autogen.agentchat import AssistantAgent
llm_config = {
"config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
"seed": 42,
"temperature": 0,
}
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant.",
llm_config=llm_config,
)
```

In `v0.4`

, it is similar, but you need to specify `model_client`

instead of `llm_config`

.

```
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client = OpenAIChatCompletionClient(model="gpt-4o", api_key="sk-xxx", seed=42, temperature=0)
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant.",
model_client=model_client,
)
```

However, the usage is somewhat different. In `v0.4`

, instead of calling `assistant.send`

,
you call `assistant.on_messages`

or `assistant.on_messages_stream`

to handle incoming messages.
Furthermore, the `on_messages`

and `on_messages_stream`

methods are asynchronous,
and the latter returns an async generator to stream the inner thoughts of the agent.

Here is how you can call the assistant agent in `v0.4`

directly, continuing from the above example:

```
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
async def main() -> None:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant.",
model_client=model_client,
)
cancellation_token = CancellationToken()
response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
print(response)
asyncio.run(main())
```

The [ CancellationToken](https://microsoft.github.io/reference/python/autogen_core.html#autogen_core.CancellationToken) can be used to cancel the request asynchronously
when you call

`cancellation_token.cancel()`

, which will cause the `await`

on the `on_messages`

call to raise a `CancelledError`

.Read more on [Agent Tutorial](https://microsoft.github.io/tutorial/agents.html)
and [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent).

## Multi-Modal Agent[#](https://microsoft.github.io#multi-modal-agent)

The [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) in

`v0.4`

supports multi-modal inputs if the model client supports it.
The `vision`

capability of the model client is used to determine if the agent supports multi-modal inputs.```
import asyncio
from pathlib import Path
from autogen_agentchat.messages import MultiModalMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken, Image
from autogen_ext.models.openai import OpenAIChatCompletionClient
async def main() -> None:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant.",
model_client=model_client,
)
cancellation_token = CancellationToken()
message = MultiModalMessage(
content=["Here is an image:", Image.from_file(Path("test.png"))],
source="user",
)
response = await assistant.on_messages([message], cancellation_token)
print(response)
asyncio.run(main())
```

## User Proxy[#](https://microsoft.github.io#user-proxy)

In `v0.2`

, you create a user proxy as follows:

```
from autogen.agentchat import UserProxyAgent
user_proxy = UserProxyAgent(
name="user_proxy",
human_input_mode="NEVER",
max_consecutive_auto_reply=10,
code_execution_config=False,
llm_config=False,
)
```

This user proxy would take input from the user through console, and would terminate if the incoming message ends with “TERMINATE”.

In `v0.4`

, a user proxy is simply an agent that takes user input only, there is no
other special configuration needed. You can create a user proxy as follows:

```
from autogen_agentchat.agents import UserProxyAgent
user_proxy = UserProxyAgent("user_proxy")
```

See [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)
for more details and how to customize the input function with timeout.

## Conversable Agent and Register Reply[#](https://microsoft.github.io#conversable-agent-and-register-reply)

In `v0.2`

, you can create a conversable agent and register a reply function as follows:

```
from typing import Any, Dict, List, Optional, Tuple, Union
from autogen.agentchat import ConversableAgent
llm_config = {
"config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
"seed": 42,
"temperature": 0,
}
conversable_agent = ConversableAgent(
name="conversable_agent",
system_message="You are a helpful assistant.",
llm_config=llm_config,
code_execution_config={"work_dir": "coding"},
human_input_mode="NEVER",
max_consecutive_auto_reply=10,
)
def reply_func(
recipient: ConversableAgent,
messages: Optional[List[Dict]] = None,
sender: Optional[Agent] = None,
config: Optional[Any] = None,
) -> Tuple[bool, Union[str, Dict, None]]:
# Custom reply logic here
return True, "Custom reply"
# Register the reply function
conversable_agent.register_reply([ConversableAgent], reply_func, position=0)
# NOTE: An async reply function will only be invoked with async send.
```

Rather than guessing what the `reply_func`

does, all its parameters,
and what the `position`

should be, in `v0.4`

, we can simply create a custom agent
and implement the `on_messages`

, `on_reset`

, and `produced_message_types`

methods.

```
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response
class CustomAgent(BaseChatAgent):
async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
return Response(chat_message=TextMessage(content="Custom reply", source=self.name))
async def on_reset(self, cancellation_token: CancellationToken) -> None:
pass
@property
def produced_message_types(self) -> Sequence[type[ChatMessage]]:
return (TextMessage,)
```

You can then use the custom agent in the same way as the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent).
See

[Custom Agent Tutorial](https://microsoft.github.io/custom-agents.html)for more details.

## Save and Load Agent State[#](https://microsoft.github.io#save-and-load-agent-state)

In `v0.2`

there is no built-in way to save and load an agent’s state: you need
to implement it yourself by exporting the `chat_messages`

attribute of `ConversableAgent`

and importing it back through the `chat_messages`

parameter.

In `v0.4`

, you can call `save_state`

and `load_state`

methods on agents to save and load their state.

```
import asyncio
import json
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
async def main() -> None:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant.",
model_client=model_client,
)
cancellation_token = CancellationToken()
response = await assistant.on_messages([TextMessage(content="Hello!", source="user")], cancellation_token)
print(response)
# Save the state.
state = await assistant.save_state()
# (Optional) Write state to disk.
with open("assistant_state.json", "w") as f:
json.dump(state, f)
# (Optional) Load it back from disk.
with open("assistant_state.json", "r") as f:
state = json.load(f)
print(state) # Inspect the state, which contains the chat history.
# Carry on the chat.
response = await assistant.on_messages([TextMessage(content="Tell me a joke.", source="user")], cancellation_token)
print(response)
# Load the state, resulting the agent to revert to the previous state before the last message.
await assistant.load_state(state)
# Carry on the same chat again.
response = await assistant.on_messages([TextMessage(content="Tell me a joke.", source="user")], cancellation_token)
asyncio.run(main())
```

You can also call `save_state`

and `load_state`

on any teams, such as [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat)
to save and load the state of the entire team.

## Two-Agent Chat[#](https://microsoft.github.io#two-agent-chat)

In `v0.2`

, you can create a two-agent chat for code execution as follows:

```
from autogen.coding import LocalCommandLineCodeExecutor
from autogen.agentchat import AssistantAgent, UserProxyAgent
llm_config = {
"config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
"seed": 42,
"temperature": 0,
}
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.",
llm_config=llm_config,
is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
)
user_proxy = UserProxyAgent(
name="user_proxy",
human_input_mode="NEVER",
max_consecutive_auto_reply=10,
code_execution_config={"code_executor": LocalCommandLineCodeExecutor(work_dir="coding")},
llm_config=False,
is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("TERMINATE"),
)
chat_result = user_proxy.initiate_chat(assistant, message="Write a python script to print 'Hello, world!'")
# Intermediate messages are printed to the console directly.
print(chat_result)
```

To get the same behavior in `v0.4`

, you can use the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent)
and

[together in a](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.CodeExecutorAgent)

`CodeExecutorAgent`

[.](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat)

`RoundRobinGroupChat`

```
import asyncio
from autogen_agentchat.agents import AssistantAgent, CodeExecutorAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination
from autogen_agentchat.ui import Console
from autogen_ext.code_executors.local import LocalCommandLineCodeExecutor
from autogen_ext.models.openai import OpenAIChatCompletionClient
async def main() -> None:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant. Write all code in python. Reply only 'TERMINATE' if the task is done.",
model_client=model_client,
)
code_executor = CodeExecutorAgent(
name="code_executor",
code_executor=LocalCommandLineCodeExecutor(work_dir="coding"),
)
# The termination condition is a combination of text termination and max message termination, either of which will cause the chat to terminate.
termination = TextMentionTermination("TERMINATE") | MaxMessageTermination(10)
# The group chat will alternate between the assistant and the code executor.
group_chat = RoundRobinGroupChat([assistant, code_executor], termination_condition=termination)
# `run_stream` returns an async generator to stream the intermediate messages.
stream = group_chat.run_stream(task="Write a python script to print 'Hello, world!'")
# `Console` is a simple UI to display the stream.
await Console(stream)
asyncio.run(main())
```

## Tool Use[#](https://microsoft.github.io#tool-use)

In `v0.2`

, to create a tool use chatbot, you must have two agents, one for calling the tool and one for executing the tool.
You need to initiate a two-agent chat for every user request.

```
from autogen.agentchat import AssistantAgent, UserProxyAgent, register_function
llm_config = {
"config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
"seed": 42,
"temperature": 0,
}
tool_caller = AssistantAgent(
name="tool_caller",
system_message="You are a helpful assistant. You can call tools to help user.",
llm_config=llm_config,
max_consecutive_auto_reply=1, # Set to 1 so that we return to the application after each assistant reply as we are building a chatbot.
)
tool_executor = UserProxyAgent(
name="tool_executor",
human_input_mode="NEVER",
code_execution_config=False,
llm_config=False,
)
def get_weather(city: str) -> str:
return f"The weather in {city} is 72 degree and sunny."
# Register the tool function to the tool caller and executor.
register_function(get_weather, caller=tool_caller, executor=tool_executor)
while True:
user_input = input("User: ")
if user_input == "exit":
break
chat_result = tool_executor.initiate_chat(
tool_caller,
message=user_input,
summary_method="reflection_with_llm", # To let the model reflect on the tool use, set to "last_msg" to return the tool call result directly.
)
print("Assistant:", chat_result.summary)
```

In `v0.4`

, you really just need one agent – the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) – to handle
both the tool calling and tool execution.

```
import asyncio
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
def get_weather(city: str) -> str: # Async tool is possible too.
return f"The weather in {city} is 72 degree and sunny."
async def main() -> None:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant. You can call tools to help user.",
model_client=model_client,
tools=[get_weather],
reflect_on_tool_use=True, # Set to True to have the model reflect on the tool use, set to False to return the tool call result directly.
)
while True:
user_input = input("User: ")
if user_input == "exit":
break
response = await assistant.on_messages([TextMessage(content=user_input, source="user")], CancellationToken())
print("Assistant:", response.chat_message.content)
asyncio.run(main())
```

When using tool-equipped agents inside a group chat such as
[ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat),
you simply do the same as above to add tools to the agents, and create a
group chat with the agents.

## Chat Result[#](https://microsoft.github.io#chat-result)

In `v0.2`

, you get a `ChatResult`

object from the `initiate_chat`

method.
For example:

```
chat_result = tool_executor.initiate_chat(
tool_caller,
message=user_input,
summary_method="reflection_with_llm",
)
print(chat_result.summary) # Get LLM-reflected summary of the chat.
print(chat_result.chat_history) # Get the chat history.
print(chat_result.cost) # Get the cost of the chat.
print(chat_result.human_input) # Get the human input solicited by the chat.
```

See [ChatResult Docs](https://microsoft.github.io/autogen/0.2/docs/reference/agentchat/chat#chatresult)
for more details.

In `v0.4`

, you get a [ TaskResult](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult) object from a

`run`

or `run_stream`

method.
The [object contains the](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult)

`TaskResult`

`messages`

which is the message history
of the chat, including both agents’ private (tool calls, etc.) and public messages.There are some notable differences between [ TaskResult](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult) and

`ChatResult`

:The

`messages`

list inuses different message format than the`TaskResult`

`ChatResult.chat_history`

list.There is no

`summary`

field. It is up to the application to decide how to summarize the chat using the`messages`

list.`human_input`

is not provided in theobject, as the user input can be extracted from the`TaskResult`

`messages`

list by filtering with the`source`

field.`cost`

is not provided in theobject, however, you can calculate the cost based on token usage. It would be a great community extension to add cost calculation. See`TaskResult`

[community extensions](https://microsoft.github.io/extensions-user-guide/discover.html).

## Conversion between v0.2 and v0.4 Messages[#](https://microsoft.github.io#conversion-between-v0-2-and-v0-4-messages)

You can use the following conversion functions to convert between a v0.4 message in
[ autogen_agentchat.base.TaskResult.messages](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult.messages) and a v0.2 message in

`ChatResult.chat_history`

.```
from typing import Any, Dict, List, Literal
from autogen_agentchat.messages import (
AgentEvent,
ChatMessage,
HandoffMessage,
MultiModalMessage,
StopMessage,
TextMessage,
ToolCallExecutionEvent,
ToolCallRequestEvent,
ToolCallSummaryMessage,
)
from autogen_core import FunctionCall, Image
from autogen_core.models import FunctionExecutionResult
def convert_to_v02_message(
message: AgentEvent | ChatMessage,
role: Literal["assistant", "user", "tool"],
image_detail: Literal["auto", "high", "low"] = "auto",
) -> Dict[str, Any]:
"""Convert a v0.4 AgentChat message to a v0.2 message.
Args:
message (AgentEvent | ChatMessage): The message to convert.
role (Literal["assistant", "user", "tool"]): The role of the message.
image_detail (Literal["auto", "high", "low"], optional): The detail level of image content in multi-modal message. Defaults to "auto".
Returns:
Dict[str, Any]: The converted AutoGen v0.2 message.
"""
v02_message: Dict[str, Any] = {}
if isinstance(message, TextMessage | StopMessage | HandoffMessage | ToolCallSummaryMessage):
v02_message = {"content": message.content, "role": role, "name": message.source}
elif isinstance(message, MultiModalMessage):
v02_message = {"content": [], "role": role, "name": message.source}
for modal in message.content:
if isinstance(modal, str):
v02_message["content"].append({"type": "text", "text": modal})
elif isinstance(modal, Image):
v02_message["content"].append(modal.to_openai_format(detail=image_detail))
else:
raise ValueError(f"Invalid multimodal message content: {modal}")
elif isinstance(message, ToolCallRequestEvent):
v02_message = {"tool_calls": [], "role": "assistant", "content": None, "name": message.source}
for tool_call in message.content:
v02_message["tool_calls"].append(
{
"id": tool_call.id,
"type": "function",
"function": {"name": tool_call.name, "args": tool_call.arguments},
}
)
elif isinstance(message, ToolCallExecutionEvent):
tool_responses: List[Dict[str, str]] = []
for tool_result in message.content:
tool_responses.append(
{
"tool_call_id": tool_result.call_id,
"role": "tool",
"content": tool_result.content,
}
)
content = "\n\n".join([response["content"] for response in tool_responses])
v02_message = {"tool_responses": tool_responses, "role": "tool", "content": content}
else:
raise ValueError(f"Invalid message type: {type(message)}")
return v02_message
def convert_to_v04_message(message: Dict[str, Any]) -> AgentEvent | ChatMessage:
"""Convert a v0.2 message to a v0.4 AgentChat message."""
if "tool_calls" in message:
tool_calls: List[FunctionCall] = []
for tool_call in message["tool_calls"]:
tool_calls.append(
FunctionCall(
id=tool_call["id"],
name=tool_call["function"]["name"],
arguments=tool_call["function"]["args"],
)
)
return ToolCallRequestEvent(source=message["name"], content=tool_calls)
elif "tool_responses" in message:
tool_results: List[FunctionExecutionResult] = []
for tool_response in message["tool_responses"]:
tool_results.append(
FunctionExecutionResult(
call_id=tool_response["tool_call_id"],
content=tool_response["content"],
is_error=False,
name=tool_response["name"],
)
)
return ToolCallExecutionEvent(source="tools", content=tool_results)
elif isinstance(message["content"], list):
content: List[str | Image] = []
for modal in message["content"]: # type: ignore
if modal["type"] == "text": # type: ignore
content.append(modal["text"]) # type: ignore
else:
content.append(Image.from_uri(modal["image_url"]["url"])) # type: ignore
return MultiModalMessage(content=content, source=message["name"])
elif isinstance(message["content"], str):
return TextMessage(content=message["content"], source=message["name"])
else:
raise ValueError(f"Unable to convert message: {message}")
```

## Group Chat[#](https://microsoft.github.io#group-chat)

In `v0.2`

, you need to create a `GroupChat`

class and pass it into a
`GroupChatManager`

, and have a participant that is a user proxy to initiate the chat.
For a simple scenario of a writer and a critic, you can do the following:

```
from autogen.agentchat import AssistantAgent, GroupChat, GroupChatManager
llm_config = {
"config_list": [{"model": "gpt-4o", "api_key": "sk-xxx"}],
"seed": 42,
"temperature": 0,
}
writer = AssistantAgent(
name="writer",
description="A writer.",
system_message="You are a writer.",
llm_config=llm_config,
is_termination_msg=lambda x: x.get("content", "").rstrip().endswith("APPROVE"),
)
critic = AssistantAgent(
name="critic",
description="A critic.",
system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.",
llm_config=llm_config,
)
# Create a group chat with the writer and critic.
groupchat = GroupChat(agents=[writer, critic], messages=[], max_round=12)
# Create a group chat manager to manage the group chat, use round-robin selection method.
manager = GroupChatManager(groupchat=groupchat, llm_config=llm_config, speaker_selection_method="round_robin")
# Initiate the chat with the editor, intermediate messages are printed to the console directly.
result = editor.initiate_chat(
manager,
message="Write a short story about a robot that discovers it has feelings.",
)
print(result.summary)
```

In `v0.4`

, you can use the [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) to achieve the same behavior.

```
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
async def main() -> None:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
writer = AssistantAgent(
name="writer",
description="A writer.",
system_message="You are a writer.",
model_client=model_client,
)
critic = AssistantAgent(
name="critic",
description="A critic.",
system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.",
model_client=model_client,
)
# The termination condition is a text termination, which will cause the chat to terminate when the text "APPROVE" is received.
termination = TextMentionTermination("APPROVE")
# The group chat will alternate between the writer and the critic.
group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination, max_turns=12)
# `run_stream` returns an async generator to stream the intermediate messages.
stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
# `Console` is a simple UI to display the stream.
await Console(stream)
asyncio.run(main())
```

For LLM-based speaker selection, you can use the [ SelectorGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat) instead.
See

[Selector Group Chat Tutorial](https://microsoft.github.io/selector-group-chat.html)and

[for more details.](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat)

`SelectorGroupChat`


Note: In`v0.4`

, you do not need to register functions on a user proxy to use tools in a group chat. You can simply pass the tool functions to the[as shown in the]`AssistantAgent`

[Tool Use]section. The agent will automatically call the tools when needed. If your tool doesn’t output well formed response, you can use the`reflect_on_tool_use`

parameter to have the model reflect on the tool use.

## Group Chat with Resume[#](https://microsoft.github.io#group-chat-with-resume)

In `v0.2`

, group chat with resume is a bit complicated. You need to explicitly
save the group chat messages and load them back when you want to resume the chat.
See [Resuming Group Chat in v0.2](https://microsoft.github.io/autogen/0.2/docs/topics/groupchat/resuming_groupchat) for more details.

In `v0.4`

, you can simply call `run`

or `run_stream`

again with the same group chat object to resume the chat. To export and load the state, you can use
`save_state`

and `load_state`

methods.

```
import asyncio
import json
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
def create_team() -> RoundRobinGroupChat:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
writer = AssistantAgent(
name="writer",
description="A writer.",
system_message="You are a writer.",
model_client=model_client,
)
critic = AssistantAgent(
name="critic",
description="A critic.",
system_message="You are a critic, provide feedback on the writing. Reply only 'APPROVE' if the task is done.",
model_client=model_client,
)
# The termination condition is a text termination, which will cause the chat to terminate when the text "APPROVE" is received.
termination = TextMentionTermination("APPROVE")
# The group chat will alternate between the writer and the critic.
group_chat = RoundRobinGroupChat([writer, critic], termination_condition=termination)
return group_chat
async def main() -> None:
# Create team.
group_chat = create_team()
# `run_stream` returns an async generator to stream the intermediate messages.
stream = group_chat.run_stream(task="Write a short story about a robot that discovers it has feelings.")
# `Console` is a simple UI to display the stream.
await Console(stream)
# Save the state of the group chat and all participants.
state = await group_chat.save_state()
with open("group_chat_state.json", "w") as f:
json.dump(state, f)
# Create a new team with the same participants configuration.
group_chat = create_team()
# Load the state of the group chat and all participants.
with open("group_chat_state.json", "r") as f:
state = json.load(f)
await group_chat.load_state(state)
# Resume the chat.
stream = group_chat.run_stream(task="Translate the story into Chinese.")
await Console(stream)
asyncio.run(main())
```

## Save and Load Group Chat State[#](https://microsoft.github.io#save-and-load-group-chat-state)

In `v0.2`

, you need to explicitly save the group chat messages and load them back when you want to resume the chat.

In `v0.4`

, you can simply call `save_state`

and `load_state`

methods on the group chat object.
See [Group Chat with Resume](https://microsoft.github.io#group-chat-with-resume) for an example.

## Group Chat with Tool Use[#](https://microsoft.github.io#group-chat-with-tool-use)

In `v0.2`

group chat, when tools are involved, you need to register the tool functions on a user proxy,
and include the user proxy in the group chat. The tool calls made by other agents
will be routed to the user proxy to execute.

We have observed numerous issues with this approach, such as the the tool call routing not working as expected, and the tool call request and result cannot be accepted by models without support for function calling.

In `v0.4`

, there is no need to register the tool functions on a user proxy,
as the tools are directly executed within the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent),
which publishes the response from the tool to the group chat.
So the group chat manager does not need to be involved in routing tool calls.

See [Selector Group Chat Tutorial](https://microsoft.github.io/selector-group-chat.html) for an example
of using tools in a group chat.

## Group Chat with Custom Selector (Stateflow)[#](https://microsoft.github.io#group-chat-with-custom-selector-stateflow)

In `v0.2`

group chat, when the `speaker_selection_method`

is set to a custom function,
it can override the default selection method. This is useful for implementing
a state-based selection method.
For more details, see [Custom Sepaker Selection in v0.2](https://microsoft.github.io/autogen/0.2/docs/topics/groupchat/customized_speaker_selection).

In `v0.4`

, you can use the [ SelectorGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat) with

`selector_func`

to achieve the same behavior.
The `selector_func`

is a function that takes the current message thread of the group chat
and returns the next speaker’s name. If `None`

is returned, the LLM-based
selection method will be used.Here is an example of using the state-based selection method to implement a web search/analysis scenario.

```
import asyncio
from typing import Sequence
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.messages import AgentEvent, ChatMessage
from autogen_agentchat.teams import SelectorGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
# Note: This example uses mock tools instead of real APIs for demonstration purposes
def search_web_tool(query: str) -> str:
if "2006-2007" in query:
return """Here are the total points scored by Miami Heat players in the 2006-2007 season:
Udonis Haslem: 844 points
Dwayne Wade: 1397 points
James Posey: 550 points
...
"""
elif "2007-2008" in query:
return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2007-2008 is 214."
elif "2008-2009" in query:
return "The number of total rebounds for Dwayne Wade in the Miami Heat season 2008-2009 is 398."
return "No data found."
def percentage_change_tool(start: float, end: float) -> float:
return ((end - start) / start) * 100
def create_team() -> SelectorGroupChat:
model_client = OpenAIChatCompletionClient(model="gpt-4o")
planning_agent = AssistantAgent(
"PlanningAgent",
description="An agent for planning tasks, this agent should be the first to engage when given a new task.",
model_client=model_client,
system_message="""
You are a planning agent.
Your job is to break down complex tasks into smaller, manageable subtasks.
Your team members are:
Web search agent: Searches for information
Data analyst: Performs calculations
You only plan and delegate tasks - you do not execute them yourself.
When assigning tasks, use this format:
1. <agent> : <task>
After all tasks are complete, summarize the findings and end with "TERMINATE".
""",
)
web_search_agent = AssistantAgent(
"WebSearchAgent",
description="A web search agent.",
tools=[search_web_tool],
model_client=model_client,
system_message="""
You are a web search agent.
Your only tool is search_tool - use it to find information.
You make only one search call at a time.
Once you have the results, you never do calculations based on them.
""",
)
data_analyst_agent = AssistantAgent(
"DataAnalystAgent",
description="A data analyst agent. Useful for performing calculations.",
model_client=model_client,
tools=[percentage_change_tool],
system_message="""
You are a data analyst.
Given the tasks you have been assigned, you should analyze the data and provide results using the tools provided.
""",
)
# The termination condition is a combination of text mention termination and max message termination.
text_mention_termination = TextMentionTermination("TERMINATE")
max_messages_termination = MaxMessageTermination(max_messages=25)
termination = text_mention_termination | max_messages_termination
# The selector function is a function that takes the current message thread of the group chat
# and returns the next speaker's name. If None is returned, the LLM-based selection method will be used.
def selector_func(messages: Sequence[AgentEvent | ChatMessage]) -> str | None:
if messages[-1].source != planning_agent.name:
return planning_agent.name # Always return to the planning agent after the other agents have spoken.
return None
team = SelectorGroupChat(
[planning_agent, web_search_agent, data_analyst_agent],
model_client=OpenAIChatCompletionClient(model="gpt-4o-mini"), # Use a smaller model for the selector.
termination_condition=termination,
selector_func=selector_func,
)
return team
async def main() -> None:
team = create_team()
task = "Who was the Miami Heat player with the highest points in the 2006-2007 season, and what was the percentage change in his total rebounds between the 2007-2008 and 2008-2009 seasons?"
await Console(team.run_stream(task=task))
asyncio.run(main())
```

## Nested Chat[#](https://microsoft.github.io#nested-chat)

Nested chat allows you to nest a whole team or another agent inside an agent. This is useful for creating a hierarchical structure of agents or “information silos”, as the nested agents cannot communicate directly with other agents outside of the same group.

In `v0.2`

, nested chat is supported by using the `register_nested_chats`

method
on the `ConversableAgent`

class.
You need to specify the nested sequence of agents using dictionaries,
See [Nested Chat in v0.2](https://microsoft.github.io/autogen/0.2/docs/tutorial/conversation-patterns#nested-chats)
for more details.

In `v0.4`

, nested chat is an implementation detail of a custom agent.
You can create a custom agent that takes a team or another agent as a parameter
and implements the `on_messages`

method to trigger the nested team or agent.
It is up to the application to decide how to pass or transform the messages from
and to the nested team or agent.

The following example shows a simple nested chat that counts numbers.

```
import asyncio
from typing import Sequence
from autogen_core import CancellationToken
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.messages import TextMessage, ChatMessage
from autogen_agentchat.base import Response
class CountingAgent(BaseChatAgent):
"""An agent that returns a new number by adding 1 to the last number in the input messages."""
async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
if len(messages) == 0:
last_number = 0 # Start from 0 if no messages are given.
else:
assert isinstance(messages[-1], TextMessage)
last_number = int(messages[-1].content) # Otherwise, start from the last number.
return Response(chat_message=TextMessage(content=str(last_number + 1), source=self.name))
async def on_reset(self, cancellation_token: CancellationToken) -> None:
pass
@property
def produced_message_types(self) -> Sequence[type[ChatMessage]]:
return (TextMessage,)
class NestedCountingAgent(BaseChatAgent):
"""An agent that increments the last number in the input messages
multiple times using a nested counting team."""
def __init__(self, name: str, counting_team: RoundRobinGroupChat) -> None:
super().__init__(name, description="An agent that counts numbers.")
self._counting_team = counting_team
async def on_messages(self, messages: Sequence[ChatMessage], cancellation_token: CancellationToken) -> Response:
# Run the inner team with the given messages and returns the last message produced by the team.
result = await self._counting_team.run(task=messages, cancellation_token=cancellation_token)
# To stream the inner messages, implement `on_messages_stream` and use that to implement `on_messages`.
assert isinstance(result.messages[-1], TextMessage)
return Response(chat_message=result.messages[-1], inner_messages=result.messages[len(messages):-1])
async def on_reset(self, cancellation_token: CancellationToken) -> None:
# Reset the inner team.
await self._counting_team.reset()
@property
def produced_message_types(self) -> Sequence[type[ChatMessage]]:
return (TextMessage,)
async def main() -> None:
# Create a team of two counting agents as the inner team.
counting_agent_1 = CountingAgent("counting_agent_1", description="An agent that counts numbers.")
counting_agent_2 = CountingAgent("counting_agent_2", description="An agent that counts numbers.")
counting_team = RoundRobinGroupChat([counting_agent_1, counting_agent_2], max_turns=5)
# Create a nested counting agent that takes the inner team as a parameter.
nested_counting_agent = NestedCountingAgent("nested_counting_agent", counting_team)
# Run the nested counting agent with a message starting from 1.
response = await nested_counting_agent.on_messages([TextMessage(content="1", source="user")], CancellationToken())
assert response.inner_messages is not None
for message in response.inner_messages:
print(message)
print(response.chat_message)
asyncio.run(main())
```

You should see the following output:

```
source='counting_agent_1' models_usage=None content='2' type='TextMessage'
source='counting_agent_2' models_usage=None content='3' type='TextMessage'
source='counting_agent_1' models_usage=None content='4' type='TextMessage'
source='counting_agent_2' models_usage=None content='5' type='TextMessage'
source='counting_agent_1' models_usage=None content='6' type='TextMessage'
```

You can take a look at [ SocietyOfMindAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.SocietyOfMindAgent)
for a more complex implementation.

## Sequential Chat[#](https://microsoft.github.io#sequential-chat)

In `v0.2`

, sequential chat is supported by using the `initiate_chats`

function.
It takes input a list of dictionary configurations for each step of the sequence.
See [Sequential Chat in v0.2](https://microsoft.github.io/autogen/0.2/docs/tutorial/conversation-patterns#sequential-chats)
for more details.

Base on the feedback from the community, the `initiate_chats`

function
is too opinionated and not flexible enough to support the diverse set of scenarios that
users want to implement. We often find users struggling to get the `initiate_chats`

function
to work when they can easily glue the steps together usign basic Python code.
Therefore, in `v0.4`

, we do not provide a built-in function for sequential chat in the AgentChat API.

Instead, you can create an event-driven sequential workflow using the Core API,
and use the other components provided the AgentChat API to implement each step of the workflow.
See an example of sequential workflow in the [Core API Tutorial](https://microsoft.github.io/core-user-guide/design-patterns/sequential-workflow.html).

We recognize that the concept of workflow is at the heart of many applications, and we will provide more built-in support for workflows in the future.

## GPTAssistantAgent[#](https://microsoft.github.io#gptassistantagent)

In `v0.2`

, `GPTAssistantAgent`

is a special agent class that is backed by the OpenAI Assistant API.

In `v0.4`

, the equivalent is the [ OpenAIAssistantAgent](https://microsoft.github.io/reference/python/autogen_ext.agents.openai.html#autogen_ext.agents.openai.OpenAIAssistantAgent) class.
It supports the same set of features as the

`GPTAssistantAgent`

in `v0.2`

with
more such as customizable threads and file uploads.
See [for more details.](https://microsoft.github.io/reference/python/autogen_ext.agents.openai.html#autogen_ext.agents.openai.OpenAIAssistantAgent)

`OpenAIAssistantAgent`

## Long Context Handling[#](https://microsoft.github.io#long-context-handling)

In `v0.2`

, long context that overflows the model’s context window can be handled
by using the `transforms`

capability that is added to an `ConversableAgent`

after which is contructed.

The feedbacks from our community has led us to believe this feature is essential
and should be a built-in component of [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent), and can be used for
every custom agent.

In `v0.4`

, we introduce the [ ChatCompletionContext](https://microsoft.github.io/reference/python/autogen_core.model_context.html#autogen_core.model_context.ChatCompletionContext) base class that manages
message history and provides a virtual view of the history. Applications can use
built-in implementations such as

[to limit the message history sent to the model, or provide their own implementations that creates different virtual views.](https://microsoft.github.io/reference/python/autogen_core.model_context.html#autogen_core.model_context.BufferedChatCompletionContext)

`BufferedChatCompletionContext`

To use [ BufferedChatCompletionContext](https://microsoft.github.io/reference/python/autogen_core.model_context.html#autogen_core.model_context.BufferedChatCompletionContext) in an

[in a chatbot scenario.](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent)

`AssistantAgent`

```
import asyncio
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.agents import AssistantAgent
from autogen_core import CancellationToken
from autogen_core.model_context import BufferedChatCompletionContext
from autogen_ext.models.openai import OpenAIChatCompletionClient
async def main() -> None:
model_client = OpenAIChatCompletionClient(model="gpt-4o", seed=42, temperature=0)
assistant = AssistantAgent(
name="assistant",
system_message="You are a helpful assistant.",
model_client=model_client,
model_context=BufferedChatCompletionContext(buffer_size=10), # Model can only view the last 10 messages.
)
while True:
user_input = input("User: ")
if user_input == "exit":
break
response = await assistant.on_messages([TextMessage(content=user_input, source="user")], CancellationToken())
print("Assistant:", response.chat_message.content)
asyncio.run(main())
```

In this example, the chatbot can only read the last 10 messages in the history.

## Observability and Control[#](https://microsoft.github.io#observability-and-control)

In `v0.4`

AgentChat, you can observe the agents by using the `on_messages_stream`

method
which returns an async generator to stream the inner thoughts and actions of the agent.
For teams, you can use the `run_stream`

method to stream the inner conversation among the agents in the team.
Your application can use these streams to observe the agents and teams in real-time.

Both the `on_messages_stream`

and `run_stream`

methods takes a [ CancellationToken](https://microsoft.github.io/reference/python/autogen_core.html#autogen_core.CancellationToken) as a parameter
which can be used to cancel the output stream asynchronously and stop the agent or team.
For teams, you can also use termination conditions to stop the team when a certain condition is met.
See

[Termination Condition Tutorial](https://microsoft.github.io/tutorial/termination.html)for more details.

Unlike the `v0.2`

which comes with a special logging module, the `v0.4`

API
simply uses Python’s `logging`

module to log events such as model client calls.
See [Logging](https://microsoft.github.io/core-user-guide/framework/logging.html)
in the Core API documentation for more details.

## Code Executors[#](https://microsoft.github.io#code-executors)

The code executors in `v0.2`

and `v0.4`

are nearly identical except
the `v0.4`

executors support async API. You can also use
[ CancellationToken](https://microsoft.github.io/reference/python/autogen_core.html#autogen_core.CancellationToken) to cancel a code execution if it takes too long.
See

[Command Line Code Executors Tutorial](https://microsoft.github.io/core-user-guide/components/command-line-code-executors.html)in the Core API documentation.

We also added `AzureContainerCodeExecutor`

that can use Azure Container Apps (ACA)
dynamic sessions for code execution.
See [ACA Dynamic Sessions Code Executor Docs](https://microsoft.github.io/extensions-user-guide/azure-container-code-executor.html).
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_tutorial_index.html"></a>

<doc title="Introduction" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html">

---
title: Introduction#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/index.html
hostname: github.io
description: Tutorial for AgentChat, a high-level API for AutoGen
sitename: microsoft.github.io
date: 2024-01-01
---
# Introduction[#](https://microsoft.github.io#introduction)

This tutorial provides a step-by-step guide to using AgentChat.
Make sure you have first followed the [installation instructions](https://microsoft.github.io/installation.html)
to prepare your environment.

At any point you are stuck, feel free to ask for help on
[GitHub Discussions](https://github.com/microsoft/autogen/discussions)
or [Discord](https://aka.ms/autogen-discord).

Note

If you are coming from AutoGen v0.2, please read the [migration guide](https://microsoft.github.io/migration-guide.html).

Models

How to use LLM model clients

[./models.html](https://microsoft.github.io/models.html)

Messages

Understand the message types

[./messages.html](https://microsoft.github.io/messages.html)

Agents

Work with AgentChat agents and get started with `AssistantAgent`


[./agents.html](https://microsoft.github.io/agents.html)

Teams

Work with teams of agents and get started with [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat).

[./teams.html](https://microsoft.github.io/teams.html)

Human-in-the-Loop

Best practices for providing feedback to a team

[./human-in-the-loop.html](https://microsoft.github.io/human-in-the-loop.html)

Termination

Control a team using termination conditions

[./termination.html](https://microsoft.github.io/termination.html)

Custom Agents

Create your own agents

[./custom-agents.html](https://microsoft.github.io/custom-agents.html)

Managing State

Save and load agents and teams for persistent sessions

[./state.html](https://microsoft.github.io/state.html)
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_tutorial_models.html"></a>

<doc title="Models" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html">

---
title: Models#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/models.html
hostname: github.io
sitename: microsoft.github.io
date: 2024-06-01
---
# Models[#](https://microsoft.github.io#models)

In many cases, agents need access to LLM model services such as OpenAI, Azure OpenAI, or local models. Since there are many different providers with different APIs, `autogen-core`

implements a protocol for model clients and `autogen-ext`

implements a set of model clients for popular model services. AgentChat can use these model clients to interact with model services.

This section provides a quick overview of available model clients.
For more details on how to use them directly, please refer to [Model Clients](https://microsoft.github.io/core-user-guide/components/model-clients.html) in the Core API documentation.

Note

See [ ChatCompletionCache](https://microsoft.github.io/reference/python/autogen_ext.models.cache.html#autogen_ext.models.cache.ChatCompletionCache) for a caching wrapper to use with the following clients.

## OpenAI[#](https://microsoft.github.io#openai)

To access OpenAI models, install the `openai`

extension, which allows you to use the [ OpenAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient).

```
pip install "autogen-ext[openai]"
```

You will also need to obtain an [API key](https://platform.openai.com/account/api-keys) from OpenAI.

```
from autogen_ext.models.openai import OpenAIChatCompletionClient
openai_model_client = OpenAIChatCompletionClient(
model="gpt-4o-2024-08-06",
# api_key="sk-...", # Optional if you have an OPENAI_API_KEY environment variable set.
)
```

To test the model client, you can use the following code:

```
from autogen_core.models import UserMessage
result = await openai_model_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(result)
```

```
CreateResult(finish_reason='stop', content='The capital of France is Paris.', usage=RequestUsage(prompt_tokens=15, completion_tokens=7), cached=False, logprobs=None)
```

Note

You can use this client with models hosted on OpenAI-compatible endpoints, however, we have not tested this functionality.
See [ OpenAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient) for more information.

## Azure OpenAI[#](https://microsoft.github.io#azure-openai)

Similarly, install the `azure`

and `openai`

extensions to use the [ AzureOpenAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.AzureOpenAIChatCompletionClient).

```
pip install "autogen-ext[openai,azure]"
```

To use the client, you need to provide your deployment id, Azure Cognitive Services endpoint, api version, and model capabilities. For authentication, you can either provide an API key or an Azure Active Directory (AAD) token credential.

The following code snippet shows how to use AAD authentication.
The identity used must be assigned the [Cognitive Services OpenAI User](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/role-based-access-control#cognitive-services-openai-user) role.

```
from autogen_ext.models.openai import AzureOpenAIChatCompletionClient
from azure.identity import DefaultAzureCredential, get_bearer_token_provider
# Create the token provider
token_provider = get_bearer_token_provider(DefaultAzureCredential(), "https://cognitiveservices.azure.com/.default")
az_model_client = AzureOpenAIChatCompletionClient(
azure_deployment="{your-azure-deployment}",
model="{model-name, such as gpt-4o}",
api_version="2024-06-01",
azure_endpoint="https://{your-custom-endpoint}.openai.azure.com/",
azure_ad_token_provider=token_provider, # Optional if you choose key-based authentication.
# api_key="sk-...", # For key-based authentication.
)
```

See [here](https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/managed-identity#chat-completions) for how to use the Azure client directly or for more information.

## Azure AI Foundry[#](https://microsoft.github.io#azure-ai-foundry)

[Azure AI Foundry](https://learn.microsoft.com/en-us/azure/ai-studio/) (previously known as Azure AI Studio) offers models hosted on Azure.
To use those models, you use the [ AzureAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.azure.html#autogen_ext.models.azure.AzureAIChatCompletionClient).

You need to install the `azure`

extra to use this client.

```
pip install "autogen-ext[azure]"
```

Below is an example of using this client with the Phi-4 model from [GitHub Marketplace](https://github.com/marketplace/models).

```
import os
from autogen_core.models import UserMessage
from autogen_ext.models.azure import AzureAIChatCompletionClient
from azure.core.credentials import AzureKeyCredential
client = AzureAIChatCompletionClient(
model="Phi-4",
endpoint="https://models.inference.ai.azure.com",
# To authenticate with the model you will need to generate a personal access token (PAT) in your GitHub settings.
# Create your PAT token by following instructions here: https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/managing-your-personal-access-tokens
credential=AzureKeyCredential(os.environ["GITHUB_TOKEN"]),
model_info={
"json_output": False,
"function_calling": False,
"vision": False,
"family": "unknown",
},
)
result = await client.create([UserMessage(content="What is the capital of France?", source="user")])
print(result)
```

```
finish_reason='stop' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=14, completion_tokens=8) cached=False logprobs=None
```

## Anthropic (experimental)[#](https://microsoft.github.io#anthropic-experimental)

To use the [ AnthropicChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.anthropic.html#autogen_ext.models.anthropic.AnthropicChatCompletionClient), you need to install the

`anthropic`

extra. Underneath, it uses the `anthropic`

python sdk to access the models.
You will also need to obtain an [API key](https://console.anthropic.com)from Anthropic.

```
# !pip install -U "autogen-ext[anthropic]"
```

```
from autogen_core.models import UserMessage
from autogen_ext.models.anthropic import AnthropicChatCompletionClient
anthropic_client = AnthropicChatCompletionClient(model="claude-3-7-sonnet-20250219")
result = await anthropic_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(result)
```

```
finish_reason='stop' content="The capital of France is Paris. It's not only the political and administrative capital but also a major global center for art, fashion, gastronomy, and culture. Paris is known for landmarks such as the Eiffel Tower, the Louvre Museum, Notre-Dame Cathedral, and the Champs-Élysées." usage=RequestUsage(prompt_tokens=14, completion_tokens=73) cached=False logprobs=None thought=None
```

## Ollama (experimental)[#](https://microsoft.github.io#ollama-experimental)

[Ollama](https://ollama.com/) is a local model server that can run models locally on your machine.

Note

Small local models are typically not as capable as larger models on the cloud. For some tasks they may not perform as well and the output may be suprising.

To use Ollama, install the `ollama`

extension and use the [ OllamaChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.ollama.html#autogen_ext.models.ollama.OllamaChatCompletionClient).

```
pip install -U "autogen-ext[ollama]"
```

```
from autogen_core.models import UserMessage
from autogen_ext.models.ollama import OllamaChatCompletionClient
# Assuming your Ollama server is running locally on port 11434.
ollama_model_client = OllamaChatCompletionClient(model="llama3.2")
response = await ollama_model_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(response)
```

```
finish_reason='unknown' content='The capital of France is Paris.' usage=RequestUsage(prompt_tokens=32, completion_tokens=8) cached=False logprobs=None thought=None
```

## Gemini (experimental)[#](https://microsoft.github.io#gemini-experimental)

Gemini currently offers [an OpenAI-compatible API (beta)](https://ai.google.dev/gemini-api/docs/openai).
So you can use the [ OpenAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient) with the Gemini API.

Note

While some model providers may offer OpenAI-compatible APIs, they may still have minor differences.
For example, the `finish_reason`

field may be different in the response.

```
from autogen_core.models import UserMessage
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client = OpenAIChatCompletionClient(
model="gemini-1.5-flash-8b",
# api_key="GEMINI_API_KEY",
)
response = await model_client.create([UserMessage(content="What is the capital of France?", source="user")])
print(response)
```

```
finish_reason='stop' content='Paris\n' usage=RequestUsage(prompt_tokens=7, completion_tokens=2) cached=False logprobs=None thought=None
```

## Semantic Kernel Adapter[#](https://microsoft.github.io#semantic-kernel-adapter)

The [ SKChatCompletionAdapter](https://microsoft.github.io/reference/python/autogen_ext.models.semantic_kernel.html#autogen_ext.models.semantic_kernel.SKChatCompletionAdapter)
allows you to use Semantic kernel model clients as a

[by adapting them to the required interface.](https://microsoft.github.io/reference/python/autogen_core.models.html#autogen_core.models.ChatCompletionClient)

`ChatCompletionClient`

You need to install the relevant provider extras to use this adapter.

The list of extras that can be installed:

`semantic-kernel-anthropic`

: Install this extra to use Anthropic models.`semantic-kernel-google`

: Install this extra to use Google Gemini models.`semantic-kernel-ollama`

: Install this extra to use Ollama models.`semantic-kernel-mistralai`

: Install this extra to use MistralAI models.`semantic-kernel-aws`

: Install this extra to use AWS models.`semantic-kernel-hugging-face`

: Install this extra to use Hugging Face models.

For example, to use Anthropic models, you need to install `semantic-kernel-anthropic`

.

```
# pip install "autogen-ext[semantic-kernel-anthropic]"
```

To use this adapter, you need create a Semantic Kernel model client and pass it to the adapter.

For example, to use the Anthropic model:

```
import os
from autogen_core.models import UserMessage
from autogen_ext.models.semantic_kernel import SKChatCompletionAdapter
from semantic_kernel import Kernel
from semantic_kernel.connectors.ai.anthropic import AnthropicChatCompletion, AnthropicChatPromptExecutionSettings
from semantic_kernel.memory.null_memory import NullMemory
sk_client = AnthropicChatCompletion(
ai_model_id="claude-3-5-sonnet-20241022",
api_key=os.environ["ANTHROPIC_API_KEY"],
service_id="my-service-id", # Optional; for targeting specific services within Semantic Kernel
)
settings = AnthropicChatPromptExecutionSettings(
temperature=0.2,
)
anthropic_model_client = SKChatCompletionAdapter(
sk_client, kernel=Kernel(memory=NullMemory()), prompt_settings=settings
)
# Call the model directly.
model_result = await anthropic_model_client.create(
messages=[UserMessage(content="What is the capital of France?", source="User")]
)
print(model_result)
```

```
finish_reason='stop' content='The capital of France is Paris. It is also the largest city in France and one of the most populous metropolitan areas in Europe.' usage=RequestUsage(prompt_tokens=0, completion_tokens=0) cached=False logprobs=None
```

Read more about the [Semantic Kernel Adapter](https://microsoft.github.io/reference/python/autogen_ext.models.semantic_kernel.html).
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_tutorial_messages.html"></a>

<doc title="Messages" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/messages.html">

---
title: Messages#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/messages.html
hostname: github.io
sitename: microsoft.github.io
date: 2024-01-01
---
# Messages[#](https://microsoft.github.io#messages)

In AutoGen AgentChat, *messages* facilitate communication and information exchange with other agents, orchestrators, and applications. AgentChat supports various message types, each designed for specific purposes.

## Types of Messages[#](https://microsoft.github.io#types-of-messages)

At a high level, messages in AgentChat can be categorized into two types: agent-agent messages and an agent’s internal events and messages.

### Agent-Agent Messages[#](https://microsoft.github.io#agent-agent-messages)

AgentChat supports many message types for agent-to-agent communication. They belong to the union type [ ChatMessage](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.ChatMessage). This message type allows both text and multimodal communication and subsumes other message types, such as

[or](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.TextMessage)

`TextMessage`

[.](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.MultiModalMessage)

`MultiModalMessage`

For example, the following code snippet demonstrates how to create a text message, which accepts a string content and a string source:

```
from autogen_agentchat.messages import TextMessage
text_message = TextMessage(content="Hello, world!", source="User")
```

Similarly, the following code snippet demonstrates how to create a multimodal message, which accepts
a list of strings or [ Image](https://microsoft.github.io/reference/python/autogen_core.html#autogen_core.Image) objects:

```
from io import BytesIO
import requests
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image as AGImage
from PIL import Image
pil_image = Image.open(BytesIO(requests.get("https://picsum.photos/300/200").content))
img = AGImage(pil_image)
multi_modal_message = MultiModalMessage(content=["Can you describe the content of this image?", img], source="User")
img
```

The [ TextMessage](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.TextMessage) and

[we have created can be passed to agents directly via the](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.MultiModalMessage)

`MultiModalMessage`

[method, or as tasks given to a team](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.ChatAgent.on_messages)

`on_messages`

[method. Messages are also used in the responses of an agent. We will explain these in more detail in](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.run)

`run()`

[Agents](https://microsoft.github.io/agents.html)and

[Teams](https://microsoft.github.io/teams.html).

### Internal Events[#](https://microsoft.github.io#internal-events)

AgentChat also supports the concept of `events`

- messages that are internal to an agent. These messages are used to communicate events and information on actions *within* the agent itself, and belong to the union type [ AgentEvent](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.AgentEvent).

Examples of these include [ ToolCallRequestEvent](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.ToolCallRequestEvent), which indicates that a request was made to call a tool, and

[, which contains the results of tool calls.](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.ToolCallExecutionEvent)

`ToolCallExecutionEvent`

Typically, events are created by the agent itself and are contained in the [ inner_messages](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response.inner_messages) field of the

[returned from](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response)

`Response`

[. If you are building a custom agent and have events that you want to communicate to other entities (e.g., a UI), you can include these in the](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.ChatAgent.on_messages)

`on_messages`

[field of the](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response.inner_messages)

`inner_messages`

[. We will show examples of this in](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response)

`Response`

[Custom Agents](https://microsoft.github.io/custom-agents.html).

You can read about the full set of messages supported in AgentChat in the [ messages](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#module-autogen_agentchat.messages) module.
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_tutorial_agents.html"></a>

<doc title="Agents" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html">

---
title: Agents#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/agents.html
hostname: github.io
sitename: microsoft.github.io
date: 2024-01-01
---
# Agents[#](https://microsoft.github.io#agents)

AutoGen AgentChat provides a set of preset Agents, each with variations in how an agent might respond to messages. All agents share the following attributes and methods:

: The unique name of the agent.`name`

: The description of the agent in text.`description`

: Send the agent a sequence of`on_messages()`

get a`ChatMessage`

.`Response`

**It is important to note that agents are expected to be stateful and this method is expected to be called with new messages, not the complete history**.: Same as`on_messages_stream()`

but returns an iterator of`on_messages()`

or`AgentEvent`

followed by a`ChatMessage`

as the last item.`Response`

: Reset the agent to its initial state.`on_reset()`

and`run()`

: convenience methods that call`run_stream()`

and`on_messages()`

respectively but offer the same interface as`on_messages_stream()`

[Teams](https://microsoft.github.io/teams.html).

See [ autogen_agentchat.messages](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#module-autogen_agentchat.messages) for more information on AgentChat message types.

## Assistant Agent[#](https://microsoft.github.io#assistant-agent)

[ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) is a built-in agent that
uses a language model and has the ability to use tools.

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.messages import TextMessage
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
```

```
# Define a tool that searches the web for information.
async def web_search(query: str) -> str:
"""Find information on the web"""
return "AutoGen is a programming framework for building multi-agent applications."
# Create an agent that uses the OpenAI GPT-4o model.
model_client = OpenAIChatCompletionClient(
model="gpt-4o",
# api_key="YOUR_API_KEY",
)
agent = AssistantAgent(
name="assistant",
model_client=model_client,
tools=[web_search],
system_message="Use tools to solve tasks.",
)
```

## Getting Responses[#](https://microsoft.github.io#getting-responses)

We can use the [ on_messages()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent.on_messages) method to get the agent response to a given message.

```
async def assistant_run() -> None:
response = await agent.on_messages(
[TextMessage(content="Find information on AutoGen", source="user")],
cancellation_token=CancellationToken(),
)
print(response.inner_messages)
print(response.chat_message)
# Use asyncio.run(assistant_run()) when running in a script.
await assistant_run()
```

```
[ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=598, completion_tokens=16), content=[FunctionCall(id='call_9UWYM1CgE3ZbnJcSJavNDB79', arguments='{"query":"AutoGen"}', name='web_search')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, content=[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', call_id='call_9UWYM1CgE3ZbnJcSJavNDB79', is_error=False)], type='ToolCallExecutionEvent')]
source='assistant' models_usage=None content='AutoGen is a programming framework for building multi-agent applications.' type='ToolCallSummaryMessage'
```

The call to the [ on_messages()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent.on_messages) method
returns a

[that contains the agent’s final response in the](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response)

`Response`

[attribute, as well as a list of inner messages in the](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response.chat_message)

`chat_message`

[attribute, which stores the agent’s “thought process” that led to the final response.](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response.inner_messages)

`inner_messages`

Note

It is important to note that [ on_messages()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent.on_messages)
will update the internal state of the agent – it will add the messages to the agent’s
history. So you should call this method with new messages.

**You should not repeatedly call this method with the same messages or the complete history.**

Note

Unlike in v0.2 AgentChat, the tools are executed by the same agent directly within
the same call to [ on_messages()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent.on_messages).
By default, the agent will return the result of the tool call as the final response.

You can also call the [ run()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.run) method, which is a convenience method that calls

[. It follows the same interface as](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.on_messages)

`on_messages()`

[Teams](https://microsoft.github.io/teams.html)and returns a

[object.](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult)

`TaskResult`

## Multi-Modal Input[#](https://microsoft.github.io#multi-modal-input)

The [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) can handle multi-modal input
by providing the input as a

[.](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.MultiModalMessage)

`MultiModalMessage`

```
from io import BytesIO
import PIL
import requests
from autogen_agentchat.messages import MultiModalMessage
from autogen_core import Image
# Create a multi-modal message with random image and text.
pil_image = PIL.Image.open(BytesIO(requests.get("https://picsum.photos/300/200").content))
img = Image(pil_image)
multi_modal_message = MultiModalMessage(content=["Can you describe the content of this image?", img], source="user")
img
```

```
# Use asyncio.run(...) when running in a script.
response = await agent.on_messages([multi_modal_message], CancellationToken())
print(response.chat_message.content)
```

```
The image depicts a vintage car, likely from the 1930s or 1940s, with a sleek, classic design. The car seems to be customized or well-maintained, as indicated by its shiny exterior and lowered stance. It has a prominent grille and round headlights. There's a license plate on the front with the text "FARMER BOY." The setting appears to be a street with old-style buildings in the background, suggesting a historical or retro theme.
```

You can also use [ MultiModalMessage](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.MultiModalMessage) as a

`task`

input to the [method.](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.run)

`run()`

## Streaming Messages[#](https://microsoft.github.io#streaming-messages)

We can also stream each message as it is generated by the agent by using the
[ on_messages_stream()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent.on_messages_stream) method,
and use

[to print the messages as they appear to the console.](https://microsoft.github.io/reference/python/autogen_agentchat.ui.html#autogen_agentchat.ui.Console)

`Console`

```
async def assistant_run_stream() -> None:
# Option 1: read each message from the stream (as shown in the previous example).
# async for message in agent.on_messages_stream(
# [TextMessage(content="Find information on AutoGen", source="user")],
# cancellation_token=CancellationToken(),
# ):
# print(message)
# Option 2: use Console to print all messages as they appear.
await Console(
agent.on_messages_stream(
[TextMessage(content="Find information on AutoGen", source="user")],
cancellation_token=CancellationToken(),
),
output_stats=True, # Enable stats printing.
)
# Use asyncio.run(assistant_run_stream()) when running in a script.
await assistant_run_stream()
```

```
---------- assistant ----------
[FunctionCall(id='call_fSp5iTGVm2FKw5NIvfECSqNd', arguments='{"query":"AutoGen information"}', name='web_search')]
[Prompt tokens: 61, Completion tokens: 16]
---------- assistant ----------
[FunctionExecutionResult(content='AutoGen is a programming framework for building multi-agent applications.', call_id='call_fSp5iTGVm2FKw5NIvfECSqNd')]
---------- assistant ----------
AutoGen is a programming framework designed for building multi-agent applications. If you need more detailed information or specific aspects about AutoGen, feel free to ask!
[Prompt tokens: 93, Completion tokens: 32]
---------- Summary ----------
Number of inner messages: 2
Total prompt tokens: 154
Total completion tokens: 48
Duration: 4.30 seconds
```

The [ on_messages_stream()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent.on_messages_stream) method
returns an asynchronous generator that yields each inner message generated by the agent,
with the final item being the response message in the

[attribute.](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.Response.chat_message)

`chat_message`

From the messages, you can observe that the assistant agent utilized the `web_search`

tool to
gather information and responded based on the search results.

You can also use [ run_stream()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.run_stream) to get the same streaming behavior as

[. It follows the same interface as](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.on_messages_stream)

`on_messages_stream()`

[Teams](https://microsoft.github.io/teams.html).

## Using Tools[#](https://microsoft.github.io#using-tools)

Large Language Models (LLMs) are typically limited to generating text or code responses. However, many complex tasks benefit from the ability to use external tools that perform specific actions, such as fetching data from APIs or databases.

To address this limitation, modern LLMs can now accept a list of available tool schemas
(descriptions of tools and their arguments) and generate a tool call message.
This capability is known as **Tool Calling** or **Function Calling** and
is becoming a popular pattern in building intelligent agent-based applications.
Refer to the documentation from [OpenAI](https://platform.openai.com/docs/guides/function-calling)
and [Anthropic](https://docs.anthropic.com/en/docs/build-with-claude/tool-use) for more information about tool calling in LLMs.

In AgentChat, the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) can use tools to perform specific actions.
The

`web_search`

tool is one such tool that allows the assistant agent to search the web for information.
A custom tool can be a Python function or a subclass of the [.](https://microsoft.github.io/reference/python/autogen_core.tools.html#autogen_core.tools.BaseTool)

`BaseTool`

By default, when [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) executes a tool,
it will return the tool’s output as a string in

[in its response. If your tool does not return a well-formed string in natural language, you can add a reflection step to have the model summarize the tool’s output, by setting the](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.ToolCallSummaryMessage)

`ToolCallSummaryMessage`

`reflect_on_tool_use=True`

parameter in the [constructor.](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent)

`AssistantAgent`

### Built-in Tools[#](https://microsoft.github.io#built-in-tools)

AutoGen Extension provides a set of built-in tools that can be used with the Assistant Agent.
Head over to the [API documentation](https://microsoft.github.io/reference/index.html) for all the available tools
under the `autogen_ext.tools`

namespace. For example, you can find the following tools:

### Function Tool[#](https://microsoft.github.io#function-tool)

The [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) automatically
converts a Python function into a

[which can be used as a tool by the agent and automatically generates the tool schema from the function signature and docstring.](https://microsoft.github.io/reference/python/autogen_core.tools.html#autogen_core.tools.FunctionTool)

`FunctionTool`

The `web_search_func`

tool is an example of a function tool.
The schema is automatically generated.

```
from autogen_core.tools import FunctionTool
# Define a tool using a Python function.
async def web_search_func(query: str) -> str:
"""Find information on the web"""
return "AutoGen is a programming framework for building multi-agent applications."
# This step is automatically performed inside the AssistantAgent if the tool is a Python function.
web_search_function_tool = FunctionTool(web_search_func, description="Find information on the web")
# The schema is provided to the model during AssistantAgent's on_messages call.
web_search_function_tool.schema
```

```
{'name': 'web_search_func',
'description': 'Find information on the web',
'parameters': {'type': 'object',
'properties': {'query': {'description': 'query',
'title': 'Query',
'type': 'string'}},
'required': ['query'],
'additionalProperties': False},
'strict': False}
```

### Model Context Protocol Tools[#](https://microsoft.github.io#model-context-protocol-tools)

The [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) can also use tools that are
served from a Model Context Protocol (MCP) server
using

[.](https://microsoft.github.io/reference/python/autogen_ext.tools.mcp.html#autogen_ext.tools.mcp.mcp_server_tools)

`mcp_server_tools()`

```
from autogen_agentchat.agents import AssistantAgent
from autogen_ext.models.openai import OpenAIChatCompletionClient
from autogen_ext.tools.mcp import StdioServerParams, mcp_server_tools
# Get the fetch tool from mcp-server-fetch.
fetch_mcp_server = StdioServerParams(command="uvx", args=["mcp-server-fetch"])
tools = await mcp_server_tools(fetch_mcp_server)
# Create an agent that can use the fetch tool.
model_client = OpenAIChatCompletionClient(model="gpt-4o")
agent = AssistantAgent(name="fetcher", model_client=model_client, tools=tools, reflect_on_tool_use=True) # type: ignore
# Let the agent fetch the content of a URL and summarize it.
result = await agent.run(task="Summarize the content of https://en.wikipedia.org/wiki/Seattle")
print(result.messages[-1].content)
```

```
Seattle, located in Washington state, is the most populous city in the state and a major city in the Pacific Northwest region of the United States. It's known for its vibrant cultural scene, significant economic presence, and rich history. Here are some key points about Seattle from the Wikipedia page:
1. **History and Geography**: Seattle is situated between Puget Sound and Lake Washington, with the Cascade Range to the east and the Olympic Mountains to the west. Its history is deeply rooted in Native American heritage and its development was accelerated with the arrival of settlers in the 19th century. The city was officially incorporated in 1869.
2. **Economy**: Seattle is a major economic hub with a diverse economy anchored by sectors like aerospace, technology, and retail. It's home to influential companies such as Amazon and Starbucks, and has a significant impact on the tech industry due to companies like Microsoft and other technology enterprises in the surrounding area.
3. **Cultural Significance**: Known for its music scene, Seattle was the birthplace of grunge music in the early 1990s. It also boasts significant attractions like the Space Needle, Pike Place Market, and the Seattle Art Museum.
4. **Education and Innovation**: The city hosts important educational institutions, with the University of Washington being a leading research university. Seattle is recognized for fostering innovation and is a leader in environmental sustainability efforts.
5. **Demographics and Diversity**: Seattle is noted for its diverse population, reflected in its rich cultural tapestry. It has seen a significant increase in population, leading to urban development and changes in its social landscape.
These points highlight Seattle as a dynamic city with a significant cultural, economic, and educational influence within the United States and beyond.
```

### Langchain Tools[#](https://microsoft.github.io#langchain-tools)

You can also use tools from the Langchain library
by wrapping them in [ LangChainToolAdapter](https://microsoft.github.io/reference/python/autogen_ext.tools.langchain.html#autogen_ext.tools.langchain.LangChainToolAdapter).

```
import pandas as pd
from autogen_ext.tools.langchain import LangChainToolAdapter
from langchain_experimental.tools.python.tool import PythonAstREPLTool
df = pd.read_csv("https://raw.githubusercontent.com/pandas-dev/pandas/main/doc/data/titanic.csv")
tool = LangChainToolAdapter(PythonAstREPLTool(locals={"df": df}))
model_client = OpenAIChatCompletionClient(model="gpt-4o")
agent = AssistantAgent(
"assistant", tools=[tool], model_client=model_client, system_message="Use the `df` variable to access the dataset."
)
await Console(
agent.on_messages_stream(
[TextMessage(content="What's the average age of the passengers?", source="user")], CancellationToken()
),
output_stats=True,
)
```

```
---------- assistant ----------
[FunctionCall(id='call_BEYRkf53nBS1G2uG60wHP0zf', arguments='{"query":"df[\'Age\'].mean()"}', name='python_repl_ast')]
[Prompt tokens: 111, Completion tokens: 22]
---------- assistant ----------
[FunctionExecutionResult(content='29.69911764705882', call_id='call_BEYRkf53nBS1G2uG60wHP0zf')]
---------- assistant ----------
29.69911764705882
---------- Summary ----------
Number of inner messages: 2
Total prompt tokens: 111
Total completion tokens: 22
Duration: 0.62 seconds
```

```
Response(chat_message=ToolCallSummaryMessage(source='assistant', models_usage=None, content='29.69911764705882', type='ToolCallSummaryMessage'), inner_messages=[ToolCallRequestEvent(source='assistant', models_usage=RequestUsage(prompt_tokens=111, completion_tokens=22), content=[FunctionCall(id='call_BEYRkf53nBS1G2uG60wHP0zf', arguments='{"query":"df[\'Age\'].mean()"}', name='python_repl_ast')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='assistant', models_usage=None, content=[FunctionExecutionResult(content='29.69911764705882', call_id='call_BEYRkf53nBS1G2uG60wHP0zf')], type='ToolCallExecutionEvent')])
```

### Parallel Tool Calls[#](https://microsoft.github.io#parallel-tool-calls)

Some models support parallel tool calls, which can be useful for tasks that require multiple tools to be called simultaneously.
By default, if the model client produces multiple tool calls, [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent)
will call the tools in parallel.

You may want to disable parallel tool calls when the tools have side effects that may interfere with each other, or, when agent behavior needs to be consistent across different models. This should be done at the model client level.

For [ OpenAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient) and

[, set](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.AzureOpenAIChatCompletionClient)

`AzureOpenAIChatCompletionClient`

`parallel_tool_calls=False`

to disable parallel tool calls.```
model_client_no_parallel_tool_call = OpenAIChatCompletionClient(
model="gpt-4o",
parallel_tool_calls=False, # type: ignore
)
agent_no_parallel_tool_call = AssistantAgent(
name="assistant",
model_client=model_client_no_parallel_tool_call,
tools=[web_search],
system_message="Use tools to solve tasks.",
)
```

## Running an Agent in a Loop[#](https://microsoft.github.io#running-an-agent-in-a-loop)

The [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) executes one
step at a time: one model call, followed by one tool call (or parallel tool calls), and then
an optional reflection.

To run it in a loop, for example, running it until it stops producing
tool calls, please refer to [Single-Agent Team](https://microsoft.github.io/teams.html#single-agent-team).

## Structured Output[#](https://microsoft.github.io#structured-output)

Structured output allows models to return structured JSON text with pre-defined schema
provided by the application. Different from JSON-mode, the schema can be provided
as a [Pydantic BaseModel](https://docs.pydantic.dev/latest/concepts/models/)
class, which can also be used to validate the output.

Note

Structured output is only available for models that support it. It also
requires the model client to support structured output as well.
Currently, the [ OpenAIChatCompletionClient](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.OpenAIChatCompletionClient)
and

[support structured output.](https://microsoft.github.io/reference/python/autogen_ext.models.openai.html#autogen_ext.models.openai.AzureOpenAIChatCompletionClient)

`AzureOpenAIChatCompletionClient`

Structured output is also useful for incorporating Chain-of-Thought reasoning in the agent’s responses. See the example below for how to use structured output with the assistant agent.

```
from typing import Literal
from pydantic import BaseModel
# The response format for the agent as a Pydantic base model.
class AgentResponse(BaseModel):
thoughts: str
response: Literal["happy", "sad", "neutral"]
# Create an agent that uses the OpenAI GPT-4o model with the custom response format.
model_client = OpenAIChatCompletionClient(
model="gpt-4o",
response_format=AgentResponse, # type: ignore
)
agent = AssistantAgent(
"assistant",
model_client=model_client,
system_message="Categorize the input as happy, sad, or neutral following the JSON format.",
)
await Console(agent.run_stream(task="I am happy."))
```

```
---------- user ----------
I am happy.
---------- assistant ----------
{"thoughts":"The user explicitly states that they are happy.","response":"happy"}
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='I am happy.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=89, completion_tokens=18), content='{"thoughts":"The user explicitly states that they are happy.","response":"happy"}', type='TextMessage')], stop_reason=None)
```

## Streaming Tokens[#](https://microsoft.github.io#streaming-tokens)

You can stream the tokens generated by the model client by setting `model_client_stream=True`

.
This will cause the agent to yield [ ModelClientStreamingChunkEvent](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.ModelClientStreamingChunkEvent) messages
in

[and](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.on_messages_stream)

`on_messages_stream()`

[.](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.run_stream)

`run_stream()`

The underlying model API must support streaming tokens for this to work. Please check with your model provider to see if this is supported.

```
model_client = OpenAIChatCompletionClient(model="gpt-4o")
streaming_assistant = AssistantAgent(
name="assistant",
model_client=model_client,
system_message="You are a helpful assistant.",
model_client_stream=True, # Enable streaming tokens.
)
# Use an async function and asyncio.run() in a script.
async for message in streaming_assistant.on_messages_stream( # type: ignore
[TextMessage(content="Name two cities in South America", source="user")],
cancellation_token=CancellationToken(),
):
print(message)
```

```
source='assistant' models_usage=None content='Two' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' cities' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' South' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' America' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' are' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' Buenos' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' Aires' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' Argentina' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' and' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' São' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' Paulo' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' Brazil' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content='.' type='ModelClientStreamingChunkEvent'
Response(chat_message=TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), content='Two cities in South America are Buenos Aires in Argentina and São Paulo in Brazil.', type='TextMessage'), inner_messages=[])
```

You can see the streaming chunks in the output above. The chunks are generated by the model client and are yielded by the agent as they are received. The final response, the concatenation of all the chunks, is yielded right after the last chunk.

Similarly, [ run_stream()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.run_stream) will also yield the same streaming chunks,
followed by a full text message right after the last chunk.

```
async for message in streaming_assistant.run_stream(task="Name two cities in North America."): # type: ignore
print(message)
```

```
source='user' models_usage=None content='Name two cities in North America.' type='TextMessage'
source='assistant' models_usage=None content='Two' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' cities' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' North' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' America' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' are' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' New' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' York' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' City' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' the' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' United' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' States' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' and' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' Toronto' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' in' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content=' Canada' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=None content='.' type='ModelClientStreamingChunkEvent'
source='assistant' models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0) content='Two cities in North America are New York City in the United States and Toronto in Canada.' type='TextMessage'
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Name two cities in North America.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=0, completion_tokens=0), content='Two cities in North America are New York City in the United States and Toronto in Canada.', type='TextMessage')], stop_reason=None)
```

## Using Model Context[#](https://microsoft.github.io#using-model-context)

[ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) has a

`model_context`

parameter that can be used to pass in a [object. This allows the agent to use different model contexts, such as](https://microsoft.github.io/reference/python/autogen_core.model_context.html#autogen_core.model_context.ChatCompletionContext)

`ChatCompletionContext`

[to limit the context sent to the model.](https://microsoft.github.io/reference/python/autogen_core.model_context.html#autogen_core.model_context.BufferedChatCompletionContext)

`BufferedChatCompletionContext`

By default, [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) uses
the

[which sends the full conversation history to the model. To limit the context to the last](https://microsoft.github.io/reference/python/autogen_core.model_context.html#autogen_core.model_context.UnboundedChatCompletionContext)

`UnboundedChatCompletionContext`

`n`

messages, you can use the [.](https://microsoft.github.io/reference/python/autogen_core.model_context.html#autogen_core.model_context.BufferedChatCompletionContext)

`BufferedChatCompletionContext`

```
from autogen_core.model_context import BufferedChatCompletionContext
# Create an agent that uses only the last 5 messages in the context to generate responses.
agent = AssistantAgent(
name="assistant",
model_client=model_client,
tools=[web_search],
system_message="Use tools to solve tasks.",
model_context=BufferedChatCompletionContext(buffer_size=5), # Only use the last 5 messages in the context.
)
```

## Other Preset Agents[#](https://microsoft.github.io#other-preset-agents)

The following preset agents are available:

: An agent that takes user input returns it as responses.`UserProxyAgent`

: An agent that can execute code.`CodeExecutorAgent`

: An agent that is backed by an OpenAI Assistant, with ability to use custom tools.`OpenAIAssistantAgent`

: A multi-modal agent that can search the web and visit web pages for information.`MultimodalWebSurfer`

: An agent that can search and browse local files for information.`FileSurfer`

: An agent that can watch videos for information.`VideoSurfer`


## Next Step[#](https://microsoft.github.io#next-step)

Having explored the usage of the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent), we can now proceed to the next section to learn about the teams feature in AgentChat.
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_tutorial_teams.html"></a>

<doc title="Teams" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html">

---
title: Teams#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/teams.html
hostname: github.io
sitename: microsoft.github.io
date: 2024-08-06
---
# Teams[#](https://microsoft.github.io#teams)

In this section you’ll learn how to create a *multi-agent team* (or simply team) using AutoGen. A team is a group of agents that work together to achieve a common goal.

We’ll first show you how to create and run a team. We’ll then explain how to observe the team’s behavior, which is crucial for debugging and understanding the team’s performance, and common operations to control the team’s behavior.

Note

When should you use a team? Teams are for complex tasks that require collaboration and diverse expertise. However, they also demand more scaffolding to steer compared to single agents. While AutoGen simplifies the process of working with teams, start with a single agent for simpler tasks, and transition to a multi-agent team when a single agent proves inadequate. Ensure that you have optimized your single agent with the appropriate tools and instructions before moving to a team-based approach.

## Creating a Team[#](https://microsoft.github.io#creating-a-team)

[ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) is a simple yet effective team configuration where all agents share the same context and take turns responding in a round-robin fashion. Each agent, during its turn, broadcasts its response to all other agents, ensuring that the entire team maintains a consistent context.

We will begin by creating a team with two [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) and a

[condition that stops the team when a specific word is detected in the agent’s response.](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.TextMentionTermination)

`TextMentionTermination`

The two-agent team implements the *reflection* pattern, a multi-agent design pattern where a critic agent evaluates the responses of a primary agent. Learn more about the reflection pattern using the [Core API](https://microsoft.github.io/core-user-guide/design-patterns/reflection.html).

```
import asyncio
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.base import TaskResult
from autogen_agentchat.conditions import ExternalTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_core import CancellationToken
from autogen_ext.models.openai import OpenAIChatCompletionClient
# Create an OpenAI model client.
model_client = OpenAIChatCompletionClient(
model="gpt-4o-2024-08-06",
# api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)
# Create the primary agent.
primary_agent = AssistantAgent(
"primary",
model_client=model_client,
system_message="You are a helpful AI assistant.",
)
# Create the critic agent.
critic_agent = AssistantAgent(
"critic",
model_client=model_client,
system_message="Provide constructive feedback. Respond with 'APPROVE' to when your feedbacks are addressed.",
)
# Define a termination condition that stops the task if the critic approves.
text_termination = TextMentionTermination("APPROVE")
# Create a team with the primary and critic agents.
team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=text_termination)
```

## Running a Team[#](https://microsoft.github.io#running-a-team)

Let’s call the [ run()](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.run) method
to start the team with a task.

```
# Use `asyncio.run(...)` when running in a script.
result = await team.run(task="Write a short poem about the fall season.")
print(result)
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a short poem about the fall season.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=28, completion_tokens=109), content="Leaves of amber, gold, and rust, \nDance upon the gentle gust. \nCrisp air whispers tales of old, \nAs daylight wanes, the night grows bold. \n\nPumpkin patch and apple treats, \nLaughter in the street repeats. \nSweaters warm and fires aglow, \nIt's time for nature's vibrant show. \n\nThe harvest moon ascends the sky, \nWhile geese in formation start to fly. \nAutumn speaks in colors bright, \nA fleeting grace, a pure delight. ", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=154, completion_tokens=200), content='Your poem beautifully captures the essence of the fall season with vivid imagery and a rhythmic flow. The use of descriptive language like "amber, gold, and rust" effectively paints a visual picture of the changing leaves. Phrases such as "crisp air whispers tales of old" and "daylight wanes, the night grows bold" add a poetic touch by incorporating seasonal characteristics.\n\nHowever, you might consider exploring other sensory details to deepen the reader\'s immersion. For example, mentioning the sound of crunching leaves underfoot or the scent of cinnamon and spices in the air could enhance the sensory experience.\n\nAdditionally, while the mention of "pumpkin patch and apple treats" is evocative of fall, expanding on these elements or including more personal experiences or emotions associated with the season might make the poem more relatable and engaging.\n\nOverall, you\'ve crafted a lovely poem that celebrates the beauty and traditions of autumn with grace and warmth. A few tweaks to include multisensory details could elevate it even further.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=347, completion_tokens=178), content="Thank you for the thoughtful feedback. Here's a revised version of the poem with additional sensory details:\n\nLeaves of amber, gold, and rust, \nDance upon the gentle gust. \nCrisp air whispers tales of old, \nAs daylight wanes, the night grows bold. \n\nCrunch beneath the wandering feet, \nA melody of autumn's beat. \nCinnamon and spices blend, \nIn every breeze, nostalgia sends. \n\nPumpkin patch and apple treats, \nLaughter in the street repeats. \nSweaters warm and fires aglow, \nIt's time for nature's vibrant show. \n\nThe harvest moon ascends the sky, \nWhile geese in formation start to fly. \nAutumn speaks in colors bright, \nA fleeting grace, a pure delight. \n\nI hope this version resonates even more with the spirit of fall. Thank you again for your suggestions!", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=542, completion_tokens=3), content='APPROVE', type='TextMessage')], stop_reason="Text 'APPROVE' mentioned")
```

The team runs the agents until the termination condition was met.
In this case, the team ran agents following a round-robin order until the the
termination condition was met when the word “APPROVE” was detected in the
agent’s response.
When the team stops, it returns a [ TaskResult](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult) object with all the messages produced by the agents in the team.

## Observing a Team[#](https://microsoft.github.io#observing-a-team)

Similar to the agent’s [ on_messages_stream()](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.on_messages_stream) method, you can stream the team’s messages while it is running by calling the

[method. This method returns a generator that yields messages produced by the agents in the team as they are generated, with the final item being the](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.run_stream)

`run_stream()`

[object.](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult)

`TaskResult`

```
# When running inside a script, use a async main function and call it from `asyncio.run(...)`.
await team.reset() # Reset the team for a new task.
async for message in team.run_stream(task="Write a short poem about the fall season."): # type: ignore
if isinstance(message, TaskResult):
print("Stop Reason:", message.stop_reason)
else:
print(message)
```

```
source='user' models_usage=None content='Write a short poem about the fall season.' type='TextMessage'
source='primary' models_usage=RequestUsage(prompt_tokens=28, completion_tokens=105) content="Leaves descend in golden dance, \nWhispering secrets as they fall, \nCrisp air brings a gentle trance, \nHeralding Autumn's call. \n\nPumpkins glow with orange light, \nFields wear a cloak of amber hue, \nDays retreat to longer night, \nSkies shift to deeper blue. \n\nWinds carry scents of earth and pine, \nSweaters wrap us, warm and tight, \nNature's canvas, bold design, \nIn Fall's embrace, we find delight. " type='TextMessage'
source='critic' models_usage=RequestUsage(prompt_tokens=150, completion_tokens=226) content='Your poem beautifully captures the essence of fall with vivid imagery and a soothing rhythm. The imagery of leaves descending, pumpkins glowing, and fields cloaked in amber hues effectively paints a picture of the autumn season. The use of contrasting elements like "Days retreat to longer night" and "Sweaters wrap us, warm and tight" provides a nice balance between the cold and warmth associated with the season. Additionally, the personification of autumn through phrases like "Autumn\'s call" and "Nature\'s canvas, bold design" adds depth to the depiction of fall.\n\nTo enhance the poem further, you might consider focusing on the soundscape of fall, such as the rustling of leaves or the distant call of migrating birds, to engage readers\' auditory senses. Also, varying the line lengths slightly could add a dynamic flow to the reading experience.\n\nOverall, your poem is engaging and effectively encapsulates the beauty and transition of fall. With a few adjustments to explore other sensory details, it could become even more immersive. \n\nIf you incorporate some of these suggestions or find another way to expand the sensory experience, please share your update!' type='TextMessage'
source='primary' models_usage=RequestUsage(prompt_tokens=369, completion_tokens=143) content="Thank you for the thoughtful critique and suggestions. Here's a revised version of the poem with added attention to auditory senses and varied line lengths:\n\nLeaves descend in golden dance, \nWhisper secrets in their fall, \nBreezes hum a gentle trance, \nHeralding Autumn's call. \n\nPumpkins glow with orange light, \nAmber fields beneath wide skies, \nDays retreat to longer night, \nChill winds and distant cries. \n\nRustling whispers of the trees, \nSweaters wrap us, snug and tight, \nNature's canvas, bold and free, \nIn Fall's embrace, pure delight. \n\nI appreciate your feedback and hope this version better captures the sensory richness of the season!" type='TextMessage'
source='critic' models_usage=RequestUsage(prompt_tokens=529, completion_tokens=160) content='Your revised poem is a beautiful enhancement of the original. By incorporating auditory elements such as "Breezes hum" and "Rustling whispers of the trees," you\'ve added an engaging soundscape that draws the reader deeper into the experience of fall. The varied line lengths work well to create a more dynamic rhythm throughout the poem, adding interest and variety to each stanza.\n\nThe succinct, yet vivid, lines of "Chill winds and distant cries" wonderfully evoke the atmosphere of the season, adding a touch of mystery and depth. The final stanza wraps up the poem nicely, celebrating the complete sensory embrace of fall with lines like "Nature\'s canvas, bold and free."\n\nYou\'ve successfully infused more sensory richness into the poem, enhancing its overall emotional and atmospheric impact. Great job on the revisions!\n\nAPPROVE' type='TextMessage'
Stop Reason: Text 'APPROVE' mentioned
```

As demonstrated in the example above, you can determine the reason why the team stopped by checking the [ stop_reason](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TaskResult.stop_reason) attribute.

The [ Console()](https://microsoft.github.io/reference/python/autogen_agentchat.ui.html#autogen_agentchat.ui.Console) method provides a convenient way to print messages to the console with proper formatting.

```
await team.reset() # Reset the team for a new task.
await Console(team.run_stream(task="Write a short poem about the fall season.")) # Stream the messages to the console.
```

```
---------- user ----------
Write a short poem about the fall season.
---------- primary ----------
Golden leaves in crisp air dance,
Whispering tales as they prance.
Amber hues paint the ground,
Nature's symphony all around.
Sweaters hug with tender grace,
While pumpkins smile, a warm embrace.
Chill winds hum through towering trees,
A vibrant tapestry in the breeze.
Harvest moons in twilight glow,
Casting magic on fields below.
Fall's embrace, a gentle call,
To savor beauty before snowfalls.
[Prompt tokens: 28, Completion tokens: 99]
---------- critic ----------
Your poem beautifully captures the essence of the fall season, creating a vivid and cozy atmosphere. The imagery of golden leaves and amber hues paints a picturesque scene that many can easily relate to. I particularly appreciate the personification of pumpkins and the gentle embrace of sweaters, which adds warmth to your verses.
To enhance the poem further, you might consider adding more sensory details to make the reader feel even more immersed in the experience. For example, including specific sounds, scents, or textures could deepen the connection to autumn's ambiance. Additionally, you could explore the emotional transitions as the season prepares for winter to provide a reflective element to the piece.
Overall, it's a lovely and evocative depiction of fall, evoking feelings of comfort and appreciation for nature's changing beauty. Great work!
[Prompt tokens: 144, Completion tokens: 157]
---------- primary ----------
Thank you for your thoughtful feedback! I'm glad you enjoyed the imagery and warmth in the poem. To enhance the sensory experience and emotional depth, here's a revised version incorporating your suggestions:
---
Golden leaves in crisp air dance,
Whispering tales as they prance.
Amber hues paint the crunchy ground,
Nature's symphony all around.
Sweaters hug with tender grace,
While pumpkins grin, a warm embrace.
Chill winds hum through towering trees,
Crackling fires warm the breeze.
Apples in the orchard's glow,
Sweet cider scents that overflow.
Crunch of paths beneath our feet,
Cinnamon spice and toasty heat.
Harvest moons in twilight's glow,
Casting magic on fields below.
Fall's embrace, a gentle call,
Reflects on life's inevitable thaw.
---
I hope this version enhances the sensory and emotional elements of the season. Thank you again for your insights!
[Prompt tokens: 294, Completion tokens: 195]
---------- critic ----------
APPROVE
[Prompt tokens: 506, Completion tokens: 4]
---------- Summary ----------
Number of messages: 5
Finish reason: Text 'APPROVE' mentioned
Total prompt tokens: 972
Total completion tokens: 455
Duration: 11.78 seconds
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a short poem about the fall season.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=28, completion_tokens=99), content="Golden leaves in crisp air dance, \nWhispering tales as they prance. \nAmber hues paint the ground, \nNature's symphony all around. \n\nSweaters hug with tender grace, \nWhile pumpkins smile, a warm embrace. \nChill winds hum through towering trees, \nA vibrant tapestry in the breeze. \n\nHarvest moons in twilight glow, \nCasting magic on fields below. \nFall's embrace, a gentle call, \nTo savor beauty before snowfalls. ", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=144, completion_tokens=157), content="Your poem beautifully captures the essence of the fall season, creating a vivid and cozy atmosphere. The imagery of golden leaves and amber hues paints a picturesque scene that many can easily relate to. I particularly appreciate the personification of pumpkins and the gentle embrace of sweaters, which adds warmth to your verses. \n\nTo enhance the poem further, you might consider adding more sensory details to make the reader feel even more immersed in the experience. For example, including specific sounds, scents, or textures could deepen the connection to autumn's ambiance. Additionally, you could explore the emotional transitions as the season prepares for winter to provide a reflective element to the piece.\n\nOverall, it's a lovely and evocative depiction of fall, evoking feelings of comfort and appreciation for nature's changing beauty. Great work!", type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=294, completion_tokens=195), content="Thank you for your thoughtful feedback! I'm glad you enjoyed the imagery and warmth in the poem. To enhance the sensory experience and emotional depth, here's a revised version incorporating your suggestions:\n\n---\n\nGolden leaves in crisp air dance, \nWhispering tales as they prance. \nAmber hues paint the crunchy ground, \nNature's symphony all around. \n\nSweaters hug with tender grace, \nWhile pumpkins grin, a warm embrace. \nChill winds hum through towering trees, \nCrackling fires warm the breeze. \n\nApples in the orchard's glow, \nSweet cider scents that overflow. \nCrunch of paths beneath our feet, \nCinnamon spice and toasty heat. \n\nHarvest moons in twilight's glow, \nCasting magic on fields below. \nFall's embrace, a gentle call, \nReflects on life's inevitable thaw. \n\n--- \n\nI hope this version enhances the sensory and emotional elements of the season. Thank you again for your insights!", type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=506, completion_tokens=4), content='APPROVE', type='TextMessage')], stop_reason="Text 'APPROVE' mentioned")
```

## Resetting a Team[#](https://microsoft.github.io#resetting-a-team)

You can reset the team by calling the [ reset()](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.reset) method. This method will clear the team’s state, including all agents.
It will call the each agent’s

[method to clear the agent’s state.](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.ChatAgent.on_reset)

`on_reset()`

```
await team.reset() # Reset the team for the next run.
```

It is usually a good idea to reset the team if the next task is not related to the previous task. However, if the next task is related to the previous task, you don’t need to reset and you can instead resume the team.

## Stopping a Team[#](https://microsoft.github.io#stopping-a-team)

Apart from automatic termination conditions such as [ TextMentionTermination](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.TextMentionTermination)
that stops the team based on the internal state of the team, you can also stop the team
from outside by using the

[.](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.ExternalTermination)

`ExternalTermination`

Calling [ set()](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.ExternalTermination.set)
on

[will stop the team when the current agent’s turn is over. Thus, the team may not stop immediately. This allows the current agent to finish its turn and broadcast the final message to the team before the team stops, keeping the team’s state consistent.](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.ExternalTermination)

`ExternalTermination`

```
# Create a new team with an external termination condition.
external_termination = ExternalTermination()
team = RoundRobinGroupChat(
[primary_agent, critic_agent],
termination_condition=external_termination | text_termination, # Use the bitwise OR operator to combine conditions.
)
# Run the team in a background task.
run = asyncio.create_task(Console(team.run_stream(task="Write a short poem about the fall season.")))
# Wait for some time.
await asyncio.sleep(0.1)
# Stop the team.
external_termination.set()
# Wait for the team to finish.
await run
```

```
---------- user ----------
Write a short poem about the fall season.
---------- primary ----------
Leaves of amber, gold, and red,
Gently drifting from trees overhead.
Whispers of wind through the crisp, cool air,
Nature's canvas painted with care.
Harvest moons and evenings that chill,
Fields of plenty on every hill.
Sweaters wrapped tight as twilight nears,
Fall's charming embrace, as warm as it appears.
Pumpkins aglow with autumn's light,
Harvest feasts and stars so bright.
In every leaf and breeze that calls,
We find the magic of glorious fall.
[Prompt tokens: 28, Completion tokens: 114]
---------- Summary ----------
Number of messages: 2
Finish reason: External termination requested
Total prompt tokens: 28
Total completion tokens: 114
Duration: 1.71 seconds
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a short poem about the fall season.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=28, completion_tokens=114), content="Leaves of amber, gold, and red, \nGently drifting from trees overhead. \nWhispers of wind through the crisp, cool air, \nNature's canvas painted with care. \n\nHarvest moons and evenings that chill, \nFields of plenty on every hill. \nSweaters wrapped tight as twilight nears, \nFall's charming embrace, as warm as it appears. \n\nPumpkins aglow with autumn's light, \nHarvest feasts and stars so bright. \nIn every leaf and breeze that calls, \nWe find the magic of glorious fall. ", type='TextMessage')], stop_reason='External termination requested')
```

From the ouput above, you can see the team stopped because the external termination condition was met, but the speaking agent was able to finish its turn before the team stopped.

## Resuming a Team[#](https://microsoft.github.io#resuming-a-team)

Teams are stateful and maintains the conversation history and context after each run, unless you reset the team.

You can resume a team to continue from where it left off by calling the [ run()](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.run) or

[method again without a new task.](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.run_stream)

`run_stream()`

[will continue from the next agent in the round-robin order.](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat)

`RoundRobinGroupChat`

```
await Console(team.run_stream()) # Resume the team to continue the last task.
```

```
---------- critic ----------
This poem beautifully captures the essence of the fall season with vivid imagery and a soothing rhythm. The descriptions of the changing leaves, cool air, and various autumn traditions make it easy for readers to envision and feel the charm of fall. Here are a few suggestions to enhance its impact:
1. **Structure Variation**: Consider breaking some lines with a hyphen or ellipsis for dramatic effect or emphasis. For instance, “Sweaters wrapped tight as twilight nears— / Fall’s charming embrace, as warm as it appears."
2. **Sensory Details**: While the poem already evokes visual and tactile senses, incorporating other senses such as sound or smell could deepen the immersion. For example, include the scent of wood smoke or the crunch of leaves underfoot.
3. **Metaphorical Language**: Adding metaphors or similes can further enrich the imagery. For example, you might compare the leaves falling to a golden rain or the chill in the air to a gentle whisper.
Overall, it’s a lovely depiction of fall. These suggestions are minor tweaks that might elevate the reader's experience even further. Nice work!
Let me know if these feedbacks are addressed.
[Prompt tokens: 159, Completion tokens: 237]
---------- primary ----------
Thank you for the thoughtful feedback! Here’s a revised version, incorporating your suggestions:
Leaves of amber, gold—drifting like dreams,
A golden rain from trees’ canopies.
Whispers of wind—a gentle breath,
Nature’s scented tapestry embracing earth.
Harvest moons rise as evenings chill,
Fields of plenty paint every hill.
Sweaters wrapped tight as twilight nears—
Fall’s embrace, warm as whispered years.
Pumpkins aglow with autumn’s light,
Crackling leaves underfoot in flight.
In every leaf and breeze that calls,
We find the magic of glorious fall.
I hope these changes enhance the imagery and sensory experience. Thank you again for your feedback!
[Prompt tokens: 389, Completion tokens: 150]
---------- critic ----------
Your revisions have made the poem even more evocative and immersive. The use of sensory details, such as "whispers of wind" and "crackling leaves," beautifully enriches the poem, engaging multiple senses. The metaphorical language, like "a golden rain from trees’ canopies" and "Fall’s embrace, warm as whispered years," adds depth and enhances the emotional warmth of the poem. The structural variation with the inclusion of dashes effectively adds emphasis and flow.
Overall, these changes bring greater vibrancy and life to the poem, allowing readers to truly experience the wonders of fall. Excellent work on the revisions!
APPROVE
[Prompt tokens: 556, Completion tokens: 132]
---------- Summary ----------
Number of messages: 3
Finish reason: Text 'APPROVE' mentioned
Total prompt tokens: 1104
Total completion tokens: 519
Duration: 9.79 seconds
```

```
TaskResult(messages=[TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=159, completion_tokens=237), content='This poem beautifully captures the essence of the fall season with vivid imagery and a soothing rhythm. The descriptions of the changing leaves, cool air, and various autumn traditions make it easy for readers to envision and feel the charm of fall. Here are a few suggestions to enhance its impact:\n\n1. **Structure Variation**: Consider breaking some lines with a hyphen or ellipsis for dramatic effect or emphasis. For instance, “Sweaters wrapped tight as twilight nears— / Fall’s charming embrace, as warm as it appears."\n\n2. **Sensory Details**: While the poem already evokes visual and tactile senses, incorporating other senses such as sound or smell could deepen the immersion. For example, include the scent of wood smoke or the crunch of leaves underfoot.\n\n3. **Metaphorical Language**: Adding metaphors or similes can further enrich the imagery. For example, you might compare the leaves falling to a golden rain or the chill in the air to a gentle whisper.\n\nOverall, it’s a lovely depiction of fall. These suggestions are minor tweaks that might elevate the reader\'s experience even further. Nice work!\n\nLet me know if these feedbacks are addressed.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=389, completion_tokens=150), content='Thank you for the thoughtful feedback! Here’s a revised version, incorporating your suggestions: \n\nLeaves of amber, gold—drifting like dreams, \nA golden rain from trees’ canopies. \nWhispers of wind—a gentle breath, \nNature’s scented tapestry embracing earth. \n\nHarvest moons rise as evenings chill, \nFields of plenty paint every hill. \nSweaters wrapped tight as twilight nears— \nFall’s embrace, warm as whispered years. \n\nPumpkins aglow with autumn’s light, \nCrackling leaves underfoot in flight. \nIn every leaf and breeze that calls, \nWe find the magic of glorious fall. \n\nI hope these changes enhance the imagery and sensory experience. Thank you again for your feedback!', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=556, completion_tokens=132), content='Your revisions have made the poem even more evocative and immersive. The use of sensory details, such as "whispers of wind" and "crackling leaves," beautifully enriches the poem, engaging multiple senses. The metaphorical language, like "a golden rain from trees’ canopies" and "Fall’s embrace, warm as whispered years," adds depth and enhances the emotional warmth of the poem. The structural variation with the inclusion of dashes effectively adds emphasis and flow. \n\nOverall, these changes bring greater vibrancy and life to the poem, allowing readers to truly experience the wonders of fall. Excellent work on the revisions!\n\nAPPROVE', type='TextMessage')], stop_reason="Text 'APPROVE' mentioned")
```

You can see the team resumed from where it left off in the output above, and the first message is from the next agent after the last agent that spoke before the team stopped.

Let’s resume the team again with a new task while keeping the context about the previous task.

```
# The new task is to translate the same poem to Chinese Tang-style poetry.
await Console(team.run_stream(task="将这首诗用中文唐诗风格写一遍。"))
```

```
---------- user ----------
将这首诗用中文唐诗风格写一遍。
---------- primary ----------
朔风轻拂叶飘金，
枝上斜阳染秋林。
满山丰收人欢喜，
月明归途衣渐紧。
南瓜影映灯火中，
落叶沙沙伴归程。
片片秋意随风起，
秋韵悠悠心自明。
[Prompt tokens: 700, Completion tokens: 77]
---------- critic ----------
这首改编的唐诗风格诗作成功地保留了原诗的意境与情感，体现出秋季特有的氛围和美感。通过“朔风轻拂叶飘金”、“枝上斜阳染秋林”等意象，生动地描绘出了秋天的景色，与唐诗中的自然意境相呼应。且“月明归途衣渐紧”、“落叶沙沙伴归程”让人感受到秋天的安宁与温暖。
通过这些诗句，读者能够感受到秋天的惬意与宁静，勾起丰收与团圆的画面，是一次成功的翻译改编。
APPROVE
[Prompt tokens: 794, Completion tokens: 161]
---------- Summary ----------
Number of messages: 3
Finish reason: Text 'APPROVE' mentioned
Total prompt tokens: 1494
Total completion tokens: 238
Duration: 3.89 seconds
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='将这首诗用中文唐诗风格写一遍。', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=700, completion_tokens=77), content='朔风轻拂叶飘金， \n枝上斜阳染秋林。 \n满山丰收人欢喜， \n月明归途衣渐紧。 \n\n南瓜影映灯火中， \n落叶沙沙伴归程。 \n片片秋意随风起， \n秋韵悠悠心自明。 ', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=794, completion_tokens=161), content='这首改编的唐诗风格诗作成功地保留了原诗的意境与情感，体现出秋季特有的氛围和美感。通过“朔风轻拂叶飘金”、“枝上斜阳染秋林”等意象，生动地描绘出了秋天的景色，与唐诗中的自然意境相呼应。且“月明归途衣渐紧”、“落叶沙沙伴归程”让人感受到秋天的安宁与温暖。\n\n通过这些诗句，读者能够感受到秋天的惬意与宁静，勾起丰收与团圆的画面，是一次成功的翻译改编。\n\nAPPROVE', type='TextMessage')], stop_reason="Text 'APPROVE' mentioned")
```

## Aborting a Team[#](https://microsoft.github.io#aborting-a-team)

You can abort a call to [ run()](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.run) or

[during execution by setting a](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.BaseGroupChat.run_stream)

`run_stream()`

[passed to the](https://microsoft.github.io/reference/python/autogen_core.html#autogen_core.CancellationToken)

`CancellationToken`

`cancellation_token`

parameter.Different from stopping a team, aborting a team will immediately stop the team and raise a [ CancelledError](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.CancelledError) exception.

Note

The caller will get a [ CancelledError](https://docs.python.org/3/library/asyncio-exceptions.html#asyncio.CancelledError) exception when the team is aborted.

```
# Create a cancellation token.
cancellation_token = CancellationToken()
# Use another coroutine to run the team.
run = asyncio.create_task(
team.run(
task="Translate the poem to Spanish.",
cancellation_token=cancellation_token,
)
)
# Cancel the run.
cancellation_token.cancel()
try:
result = await run # This will raise a CancelledError.
except asyncio.CancelledError:
print("Task was cancelled.")
```

```
Task was cancelled.
```

## Single-Agent Team[#](https://microsoft.github.io#single-agent-team)

Often, you may want to run a single agent in a team configuration.
This is useful for running the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) in a loop
until a termination condition is met.

This is different from running the [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) using
its

[or](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.run)

`run()`

[method, which only runs the agent for one step and returns the result. See](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.BaseChatAgent.run_stream)

`run_stream()`

[for more details about a single step.](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent)

`AssistantAgent`

Here is an example of running a single agent in a [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) team configuration
with a

[condition. The task is to increment a number until it reaches 10 using a tool. The agent will keep calling the tool until the number reaches 10, and then it will return a final](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.TextMessageTermination)

`TextMessageTermination`

[which will stop the run.](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.TextMessage)

`TextMessage`

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import TextMessageTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client = OpenAIChatCompletionClient(
model="gpt-4o",
# api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
# Disable parallel tool calls for this example.
parallel_tool_calls=False, # type: ignore
)
# Create a tool for incrementing a number.
def increment_number(number: int) -> int:
"""Increment a number by 1."""
return number + 1
# Create a tool agent that uses the increment_number function.
looped_assistant = AssistantAgent(
"looped_assistant",
model_client=model_client,
tools=[increment_number], # Register the tool.
system_message="You are a helpful AI assistant, use the tool to increment the number.",
)
# Termination condition that stops the task if the agent responds with a text message.
termination_condition = TextMessageTermination("looped_assistant")
# Create a team with the looped assistant agent and the termination condition.
team = RoundRobinGroupChat(
[looped_assistant],
termination_condition=termination_condition,
)
# Run the team with a task and print the messages to the console.
async for message in team.run_stream(task="Increment the number 5 to 10."): # type: ignore
print(type(message).__name__, message)
```

```
TextMessage source='user' models_usage=None metadata={} content='Increment the number 5 to 10.' type='TextMessage'
ToolCallRequestEvent source='looped_assistant' models_usage=RequestUsage(prompt_tokens=75, completion_tokens=15) metadata={} content=[FunctionCall(id='call_qTDXSouN3MtGDqa8l0DM1ciD', arguments='{"number":5}', name='increment_number')] type='ToolCallRequestEvent'
ToolCallExecutionEvent source='looped_assistant' models_usage=None metadata={} content=[FunctionExecutionResult(content='6', name='increment_number', call_id='call_qTDXSouN3MtGDqa8l0DM1ciD', is_error=False)] type='ToolCallExecutionEvent'
ToolCallSummaryMessage source='looped_assistant' models_usage=None metadata={} content='6' type='ToolCallSummaryMessage'
ToolCallRequestEvent source='looped_assistant' models_usage=RequestUsage(prompt_tokens=103, completion_tokens=15) metadata={} content=[FunctionCall(id='call_VGZPlsFVVdyxutR63Yr087pt', arguments='{"number":6}', name='increment_number')] type='ToolCallRequestEvent'
ToolCallExecutionEvent source='looped_assistant' models_usage=None metadata={} content=[FunctionExecutionResult(content='7', name='increment_number', call_id='call_VGZPlsFVVdyxutR63Yr087pt', is_error=False)] type='ToolCallExecutionEvent'
ToolCallSummaryMessage source='looped_assistant' models_usage=None metadata={} content='7' type='ToolCallSummaryMessage'
ToolCallRequestEvent source='looped_assistant' models_usage=RequestUsage(prompt_tokens=131, completion_tokens=15) metadata={} content=[FunctionCall(id='call_VRKGPqPM9AHoef2g2kgsKwZe', arguments='{"number":7}', name='increment_number')] type='ToolCallRequestEvent'
ToolCallExecutionEvent source='looped_assistant' models_usage=None metadata={} content=[FunctionExecutionResult(content='8', name='increment_number', call_id='call_VRKGPqPM9AHoef2g2kgsKwZe', is_error=False)] type='ToolCallExecutionEvent'
ToolCallSummaryMessage source='looped_assistant' models_usage=None metadata={} content='8' type='ToolCallSummaryMessage'
ToolCallRequestEvent source='looped_assistant' models_usage=RequestUsage(prompt_tokens=159, completion_tokens=15) metadata={} content=[FunctionCall(id='call_TOUMjSCG2kVdFcw2CMeb5DYX', arguments='{"number":8}', name='increment_number')] type='ToolCallRequestEvent'
ToolCallExecutionEvent source='looped_assistant' models_usage=None metadata={} content=[FunctionExecutionResult(content='9', name='increment_number', call_id='call_TOUMjSCG2kVdFcw2CMeb5DYX', is_error=False)] type='ToolCallExecutionEvent'
ToolCallSummaryMessage source='looped_assistant' models_usage=None metadata={} content='9' type='ToolCallSummaryMessage'
ToolCallRequestEvent source='looped_assistant' models_usage=RequestUsage(prompt_tokens=187, completion_tokens=15) metadata={} content=[FunctionCall(id='call_wjq7OO9Kf5YYurWGc5lsqttJ', arguments='{"number":9}', name='increment_number')] type='ToolCallRequestEvent'
ToolCallExecutionEvent source='looped_assistant' models_usage=None metadata={} content=[FunctionExecutionResult(content='10', name='increment_number', call_id='call_wjq7OO9Kf5YYurWGc5lsqttJ', is_error=False)] type='ToolCallExecutionEvent'
ToolCallSummaryMessage source='looped_assistant' models_usage=None metadata={} content='10' type='ToolCallSummaryMessage'
TextMessage source='looped_assistant' models_usage=RequestUsage(prompt_tokens=215, completion_tokens=15) metadata={} content='The number 5 incremented to 10 is 10.' type='TextMessage'
TaskResult TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Increment the number 5 to 10.', type='TextMessage'), ToolCallRequestEvent(source='looped_assistant', models_usage=RequestUsage(prompt_tokens=75, completion_tokens=15), metadata={}, content=[FunctionCall(id='call_qTDXSouN3MtGDqa8l0DM1ciD', arguments='{"number":5}', name='increment_number')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='looped_assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='6', name='increment_number', call_id='call_qTDXSouN3MtGDqa8l0DM1ciD', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='looped_assistant', models_usage=None, metadata={}, content='6', type='ToolCallSummaryMessage'), ToolCallRequestEvent(source='looped_assistant', models_usage=RequestUsage(prompt_tokens=103, completion_tokens=15), metadata={}, content=[FunctionCall(id='call_VGZPlsFVVdyxutR63Yr087pt', arguments='{"number":6}', name='increment_number')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='looped_assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='7', name='increment_number', call_id='call_VGZPlsFVVdyxutR63Yr087pt', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='looped_assistant', models_usage=None, metadata={}, content='7', type='ToolCallSummaryMessage'), ToolCallRequestEvent(source='looped_assistant', models_usage=RequestUsage(prompt_tokens=131, completion_tokens=15), metadata={}, content=[FunctionCall(id='call_VRKGPqPM9AHoef2g2kgsKwZe', arguments='{"number":7}', name='increment_number')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='looped_assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='8', name='increment_number', call_id='call_VRKGPqPM9AHoef2g2kgsKwZe', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='looped_assistant', models_usage=None, metadata={}, content='8', type='ToolCallSummaryMessage'), ToolCallRequestEvent(source='looped_assistant', models_usage=RequestUsage(prompt_tokens=159, completion_tokens=15), metadata={}, content=[FunctionCall(id='call_TOUMjSCG2kVdFcw2CMeb5DYX', arguments='{"number":8}', name='increment_number')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='looped_assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='9', name='increment_number', call_id='call_TOUMjSCG2kVdFcw2CMeb5DYX', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='looped_assistant', models_usage=None, metadata={}, content='9', type='ToolCallSummaryMessage'), ToolCallRequestEvent(source='looped_assistant', models_usage=RequestUsage(prompt_tokens=187, completion_tokens=15), metadata={}, content=[FunctionCall(id='call_wjq7OO9Kf5YYurWGc5lsqttJ', arguments='{"number":9}', name='increment_number')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='looped_assistant', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='10', name='increment_number', call_id='call_wjq7OO9Kf5YYurWGc5lsqttJ', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='looped_assistant', models_usage=None, metadata={}, content='10', type='ToolCallSummaryMessage'), TextMessage(source='looped_assistant', models_usage=RequestUsage(prompt_tokens=215, completion_tokens=15), metadata={}, content='The number 5 incremented to 10 is 10.', type='TextMessage')], stop_reason="Text message received from 'looped_assistant'")
```

The key is to focus on the termination condition.
In this example, we use a [ TextMessageTermination](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.TextMessageTermination) condition
that stops the team when the agent stop producing

[. The team will keep running until the agent produces a](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.ToolCallSummaryMessage)

`ToolCallSummaryMessage`

[with the final result.](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.TextMessage)

`TextMessage`

You can also use other termination conditions to control the agent.
See [Termination Conditions](https://microsoft.github.io/termination.html) for more details.
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_tutorial_human-in-the-loop.html"></a>

<doc title="Human-in-the-Loop" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html">

---
title: Human-in-the-Loop#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/human-in-the-loop.html
hostname: github.io
sitename: microsoft.github.io
date: 2024-01-01
---
# Human-in-the-Loop[#](https://microsoft.github.io#human-in-the-loop)

In the previous section [Teams](https://microsoft.github.io/teams.html), we have seen how to create, observe,
and control a team of agents.
This section will focus on how to interact with the team from your application,
and provide human feedback to the team.

There are two main ways to interact with the team from your application:

During a team’s run – execution of

or`run()`

, provide feedback through a`run_stream()`

.`UserProxyAgent`

Once the run terminates, provide feedback through input to the next call to

or`run()`

.`run_stream()`


We will cover both methods in this section.

To jump straight to code samples on integration with web and UI frameworks, see the following links:

## Providing Feedback During a Run[#](https://microsoft.github.io#providing-feedback-during-a-run)

The [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent) is a special built-in agent
that acts as a proxy for a user to provide feedback to the team.

To use the [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent), you can create an instance of it
and include it in the team before running the team.
The team will decide when to call the

[to ask for feedback from the user.](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)

`UserProxyAgent`

For example in a [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) team,
the

[is called in the order in which it is passed to the team, while in a](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)

`UserProxyAgent`

[team, the selector prompt or selector function determines when the](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat)

`SelectorGroupChat`

[is called.](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)

`UserProxyAgent`

The following diagram illustrates how you can use
[ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)
to get feedback from the user during a team’s run:

The bold arrows indicates the flow of control during a team’s run:
when the team calls the [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent),
it transfers the control to the application/user, and waits for the feedback;
once the feedback is provided, the control is transferred back to the team
and the team continues its execution.

Note

When [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent) is called during a run,
it blocks the execution of the team until the user provides feedback or errors out.
This will hold up the team’s progress and put the team in an unstable state
that cannot be saved or resumed.

Due to the blocking nature of this approach, it is recommended to use it only for short interactions that require immediate feedback from the user, such as asking for approval or disapproval with a button click, or an alert requiring immediate attention otherwise failing the task.

Here is an example of how to use the [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)
in a

[for a poetry generation task:](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat)

`RoundRobinGroupChat`

```
from autogen_agentchat.agents import AssistantAgent, UserProxyAgent
from autogen_agentchat.conditions import TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
# Create the agents.
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
assistant = AssistantAgent("assistant", model_client=model_client)
user_proxy = UserProxyAgent("user_proxy", input_func=input) # Use input() to get user input from console.
# Create the termination condition which will end the conversation when the user says "APPROVE".
termination = TextMentionTermination("APPROVE")
# Create the team.
team = RoundRobinGroupChat([assistant, user_proxy], termination_condition=termination)
# Run the conversation and stream to the console.
stream = team.run_stream(task="Write a 4-line poem about the ocean.")
# Use asyncio.run(...) when running in a script.
await Console(stream)
```

```
---------- user ----------
Write a 4-line poem about the ocean.
---------- assistant ----------
In endless blue where whispers play,
The ocean's waves dance night and day.
A world of depths, both calm and wild,
Nature's heart, forever beguiled.
TERMINATE
---------- user_proxy ----------
APPROVE
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a 4-line poem about the ocean.', type='TextMessage'), TextMessage(source='assistant', models_usage=RequestUsage(prompt_tokens=46, completion_tokens=43), metadata={}, content="In endless blue where whispers play, \nThe ocean's waves dance night and day. \nA world of depths, both calm and wild, \nNature's heart, forever beguiled. \nTERMINATE", type='TextMessage'), UserInputRequestedEvent(source='user_proxy', models_usage=None, metadata={}, request_id='2622a0aa-b776-4e54-9e8f-4ecbdf14b78d', content='', type='UserInputRequestedEvent'), TextMessage(source='user_proxy', models_usage=None, metadata={}, content='APPROVE', type='TextMessage')], stop_reason="Text 'APPROVE' mentioned")
```

From the console output, you can see the team solicited feedback from the user
through `user_proxy`

to approve the generated poem.

You can provide your own input function to the [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent)
to customize the feedback process.
For example, when the team is running as a web service, you can use a custom
input function to wait for message from a web socket connection.
The following code snippet shows an example of custom input function
when using the

[FastAPI](https://fastapi.tiangolo.com/)web framework:

```
@app.websocket("/ws/chat")
async def chat(websocket: WebSocket):
await websocket.accept()
async def _user_input(prompt: str, cancellation_token: CancellationToken | None) -> str:
data = await websocket.receive_json() # Wait for user message from websocket.
message = TextMessage.model_validate(data) # Assume user message is a TextMessage.
return message.content
# Create user proxy with custom input function
# Run the team with the user proxy
# ...
```

See the [AgentChat FastAPI sample](https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_fastapi) for a complete example.

For [ChainLit](https://github.com/Chainlit/chainlit) integration with [ UserProxyAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.UserProxyAgent),
see the

[AgentChat ChainLit sample](https://github.com/microsoft/autogen/blob/main/python/samples/agentchat_chainlit).

## Providing Feedback to the Next Run[#](https://microsoft.github.io#providing-feedback-to-the-next-run)

Often times, an application or a user interacts with the team of agents in an interactive loop: the team runs until termination, the application or user provides feedback, and the team runs again with the feedback.

This approach is useful in a persisted session with asynchronous communication between the team and the application/user: Once a team finishes a run, the application saves the state of the team, puts it in a persistent storage, and resumes the team when the feedback arrives.

Note

For how to save and load the state of a team, please refer to [Managing State](https://microsoft.github.io/state.html).
This section will focus on the feedback mechanisms.

The following diagram illustrates the flow of control in this approach:

There are two ways to implement this approach:

Set the maximum number of turns so that the team always stops after the specified number of turns.

Use termination conditions such as

and`TextMentionTermination`

to allow the team to decide when to stop and give control back, given the team’s internal state.`HandoffTermination`


You can use both methods together to achieve your desired behavior.

### Using Max Turns[#](https://microsoft.github.io#using-max-turns)

This method allows you to pause the team for user input by setting a maximum number of turns. For instance, you can configure the team to stop after the first agent responds by setting `max_turns`

to 1. This is particularly useful in scenarios where continuous user engagement is required, such as in a chatbot.

To implement this, set the `max_turns`

parameter in the [ RoundRobinGroupChat()](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) constructor.

```
team = RoundRobinGroupChat([...], max_turns=1)
```

Once the team stops, the turn count will be reset. When you resume the team,
it will start from 0 again. However, the team’s internal state will be preserved,
for example, the [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) will
resume from the next agent in the list with the same conversation history.

Note

`max_turn`

is specific to the team class and is currently only supported by
[ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat),

[, and](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat)

`SelectorGroupChat`

[. When used with termination conditions, the team will stop when either condition is met.](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.Swarm)

`Swarm`

Here is an example of how to use `max_turns`

in a [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat) for a poetry generation task
with a maximum of 1 turn:

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
# Create the agents.
model_client = OpenAIChatCompletionClient(model="gpt-4o-mini")
assistant = AssistantAgent("assistant", model_client=model_client)
# Create the team setting a maximum number of turns to 1.
team = RoundRobinGroupChat([assistant], max_turns=1)
task = "Write a 4-line poem about the ocean."
while True:
# Run the conversation and stream to the console.
stream = team.run_stream(task=task)
# Use asyncio.run(...) when running in a script.
await Console(stream)
# Get the user response.
task = input("Enter your feedback (type 'exit' to leave): ")
if task.lower().strip() == "exit":
break
```

```
---------- user ----------
Write a 4-line poem about the ocean.
---------- assistant ----------
Endless waves in a dance with the shore,
Whispers of secrets in tales from the roar,
Beneath the vast sky, where horizons blend,
The ocean’s embrace is a timeless friend.
TERMINATE
[Prompt tokens: 46, Completion tokens: 48]
---------- Summary ----------
Number of messages: 2
Finish reason: Maximum number of turns 1 reached.
Total prompt tokens: 46
Total completion tokens: 48
Duration: 1.63 seconds
---------- user ----------
Can you make it about a person and its relationship with the ocean
---------- assistant ----------
She walks along the tide, where dreams intertwine,
With every crashing wave, her heart feels aligned,
In the ocean's embrace, her worries dissolve,
A symphony of solace, where her spirit evolves.
TERMINATE
[Prompt tokens: 117, Completion tokens: 49]
---------- Summary ----------
Number of messages: 2
Finish reason: Maximum number of turns 1 reached.
Total prompt tokens: 117
Total completion tokens: 49
Duration: 1.21 seconds
```

You can see that the team stopped immediately after one agent responded.

### Using Termination Conditions[#](https://microsoft.github.io#using-termination-conditions)

We have already seen several examples of termination conditions in the previous sections.
In this section, we focus on [ HandoffTermination](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.HandoffTermination)
which stops the team when an agent sends a

[message.](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.HandoffMessage)

`HandoffMessage`

Let’s create a team with a single [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) agent
with a handoff setting, and run the team with a task that requires additional input from the user
because the agent doesn’t have relevant tools to continue processing the task.

Note

The model used with [ AssistantAgent](https://microsoft.github.io/reference/python/autogen_agentchat.agents.html#autogen_agentchat.agents.AssistantAgent) must support tool call
to use the handoff feature.

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.base import Handoff
from autogen_agentchat.conditions import HandoffTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
# Create an OpenAI model client.
model_client = OpenAIChatCompletionClient(
model="gpt-4o",
# api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)
# Create a lazy assistant agent that always hands off to the user.
lazy_agent = AssistantAgent(
"lazy_assistant",
model_client=model_client,
handoffs=[Handoff(target="user", message="Transfer to user.")],
system_message="If you cannot complete the task, transfer to user. Otherwise, when finished, respond with 'TERMINATE'.",
)
# Define a termination condition that checks for handoff messages.
handoff_termination = HandoffTermination(target="user")
# Define a termination condition that checks for a specific text mention.
text_termination = TextMentionTermination("TERMINATE")
# Create a single-agent team with the lazy assistant and both termination conditions.
lazy_agent_team = RoundRobinGroupChat([lazy_agent], termination_condition=handoff_termination | text_termination)
# Run the team and stream to the console.
task = "What is the weather in New York?"
await Console(lazy_agent_team.run_stream(task=task), output_stats=True)
```

```
---------- user ----------
What is the weather in New York?
---------- lazy_assistant ----------
[FunctionCall(id='call_EAcMgrLGHdLw0e7iJGoMgxuu', arguments='{}', name='transfer_to_user')]
[Prompt tokens: 69, Completion tokens: 12]
---------- lazy_assistant ----------
[FunctionExecutionResult(content='Transfer to user.', call_id='call_EAcMgrLGHdLw0e7iJGoMgxuu')]
---------- lazy_assistant ----------
Transfer to user.
---------- Summary ----------
Number of messages: 4
Finish reason: Handoff to user from lazy_assistant detected.
Total prompt tokens: 69
Total completion tokens: 12
Duration: 0.69 seconds
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='What is the weather in New York?', type='TextMessage'), ToolCallRequestEvent(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=69, completion_tokens=12), content=[FunctionCall(id='call_EAcMgrLGHdLw0e7iJGoMgxuu', arguments='{}', name='transfer_to_user')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='lazy_assistant', models_usage=None, content=[FunctionExecutionResult(content='Transfer to user.', call_id='call_EAcMgrLGHdLw0e7iJGoMgxuu')], type='ToolCallExecutionEvent'), HandoffMessage(source='lazy_assistant', models_usage=None, target='user', content='Transfer to user.', context=[], type='HandoffMessage')], stop_reason='Handoff to user from lazy_assistant detected.')
```

You can see the team stopped due to the handoff message was detected. Let’s continue the team by providing the information the agent needs.

```
await Console(lazy_agent_team.run_stream(task="The weather in New York is sunny."))
```

```
---------- user ----------
The weather in New York is sunny.
---------- lazy_assistant ----------
Great! Enjoy the sunny weather in New York! Is there anything else you'd like to know?
---------- lazy_assistant ----------
TERMINATE
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='The weather in New York is sunny.', type='TextMessage'), TextMessage(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=110, completion_tokens=21), content="Great! Enjoy the sunny weather in New York! Is there anything else you'd like to know?", type='TextMessage'), TextMessage(source='lazy_assistant', models_usage=RequestUsage(prompt_tokens=137, completion_tokens=5), content='TERMINATE', type='TextMessage')], stop_reason="Text 'TERMINATE' mentioned")
```

You can see the team continued after the user provided the information.

Note

If you are using [ Swarm](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.Swarm) team with

[targeting user, to resume the team, you need to set the](https://microsoft.github.io/reference/python/autogen_agentchat.conditions.html#autogen_agentchat.conditions.HandoffTermination)

`HandoffTermination`

`task`

to a [with the](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.HandoffMessage)

`HandoffMessage`

`target`

set to the next agent you want to run.
See [Swarm](https://microsoft.github.io/swarm.html)for more details.
</doc>

<a id="autogen_stable_user-guide_agentchat-user-guide_tutorial_termination.html"></a>

<doc title="Termination" desc="Content from https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html">

---
title: Termination#
url: https://microsoft.github.io/autogen/stable/user-guide/agentchat-user-guide/tutorial/termination.html
hostname: github.io
sitename: microsoft.github.io
date: 2024-01-01
---
# Termination[#](https://microsoft.github.io#termination)

In the previous section, we explored how to define agents, and organize them into teams that can solve tasks. However, a run can go on forever, and in many cases, we need to know *when* to stop them. This is the role of the termination condition.

AgentChat supports several termination condition by providing a base [ TerminationCondition](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TerminationCondition) class and several implementations that inherit from it.

A termination condition is a callable that takes a sequence of [ AgentEvent](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.AgentEvent) or

[objects](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.ChatMessage)

`ChatMessage`

**since the last time the condition was called**, and returns a

[if the conversation should be terminated, or](https://microsoft.github.io/reference/python/autogen_agentchat.messages.html#autogen_agentchat.messages.StopMessage)

`StopMessage`

`None`

otherwise.
Once a termination condition has been reached, it must be reset by calling [before it can be used again.](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TerminationCondition.reset)

`reset()`

Some important things to note about termination conditions:

They are stateful but reset automatically after each run (

or`run()`

) is finished.`run_stream()`

They can be combined using the AND and OR operators.


Note

For group chat teams (i.e., [ RoundRobinGroupChat](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.RoundRobinGroupChat),

[, and](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.SelectorGroupChat)

`SelectorGroupChat`

[), the termination condition is called after each agent responds. While a response may contain multiple inner messages, the team calls its termination condition just once for all the messages from a single response. So the condition is called with the “delta sequence” of messages since the last time it was called.](https://microsoft.github.io/reference/python/autogen_agentchat.teams.html#autogen_agentchat.teams.Swarm)

`Swarm`

Built-In Termination Conditions:

: Stops after a specified number of messages have been produced, including both agent and task messages.`MaxMessageTermination`

: Stops when specific text or string is mentioned in a message (e.g., “TERMINATE”).`TextMentionTermination`

: Stops when a certain number of prompt or completion tokens are used. This requires the agents to report token usage in their messages.`TokenUsageTermination`

: Stops after a specified duration in seconds.`TimeoutTermination`

: Stops when a handoff to a specific target is requested. Handoff messages can be used to build patterns such as`HandoffTermination`

. This is useful when you want to pause the run and allow application or user to provide input when an agent hands off to them.`Swarm`

: Stops after a specific agent responds.`SourceMatchTermination`

: Enables programmatic control of termination from outside the run. This is useful for UI integration (e.g., “Stop” buttons in chat interfaces).`ExternalTermination`

: Stops when a`StopMessageTermination`

is produced by an agent.`StopMessage`

: Stops when a`TextMessageTermination`

is produced by an agent.`TextMessage`

: Stops when a`FunctionCallTermination`

containing a`ToolCallExecutionEvent`

with a matching name is produced by an agent.`FunctionExecutionResult`


## Basic Usage[#](https://microsoft.github.io#basic-usage)

To demonstrate the characteristics of termination conditions, we’ll create a team consisting of two agents: a primary agent responsible for text generation and a critic agent that reviews and provides feedback on the generated text.

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.conditions import MaxMessageTermination, TextMentionTermination
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client = OpenAIChatCompletionClient(
model="gpt-4o",
temperature=1,
# api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)
# Create the primary agent.
primary_agent = AssistantAgent(
"primary",
model_client=model_client,
system_message="You are a helpful AI assistant.",
)
# Create the critic agent.
critic_agent = AssistantAgent(
"critic",
model_client=model_client,
system_message="Provide constructive feedback for every message. Respond with 'APPROVE' to when your feedbacks are addressed.",
)
```

Let’s explore how termination conditions automatically reset after each `run`

or `run_stream`

call, allowing the team to resume its conversation from where it left off.

```
max_msg_termination = MaxMessageTermination(max_messages=3)
round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=max_msg_termination)
# Use asyncio.run(...) if you are running this script as a standalone script.
await Console(round_robin_team.run_stream(task="Write a unique, Haiku about the weather in Paris"))
```

```
---------- user ----------
Write a unique, Haiku about the weather in Paris
---------- primary ----------
Gentle rain whispers,
Cobblestones glisten softly—
Paris dreams in gray.
[Prompt tokens: 30, Completion tokens: 19]
---------- critic ----------
The Haiku captures the essence of a rainy day in Paris beautifully, and the imagery is vivid. However, it's important to ensure the use of the traditional 5-7-5 syllable structure for Haikus. Your current Haiku lines are composed of 4-7-5 syllables, which slightly deviates from the form. Consider revising the first line to fit the structure.
For example:
Soft rain whispers down,
Cobblestones glisten softly —
Paris dreams in gray.
This revision maintains the essence of your original lines while adhering to the traditional Haiku structure.
[Prompt tokens: 70, Completion tokens: 120]
---------- Summary ----------
Number of messages: 3
Finish reason: Maximum number of messages 3 reached, current message count: 3
Total prompt tokens: 100
Total completion tokens: 139
Duration: 3.34 seconds
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a unique, Haiku about the weather in Paris'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=30, completion_tokens=19), content='Gentle rain whispers, \nCobblestones glisten softly— \nParis dreams in gray.'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=70, completion_tokens=120), content="The Haiku captures the essence of a rainy day in Paris beautifully, and the imagery is vivid. However, it's important to ensure the use of the traditional 5-7-5 syllable structure for Haikus. Your current Haiku lines are composed of 4-7-5 syllables, which slightly deviates from the form. Consider revising the first line to fit the structure.\n\nFor example:\nSoft rain whispers down, \nCobblestones glisten softly — \nParis dreams in gray.\n\nThis revision maintains the essence of your original lines while adhering to the traditional Haiku structure.")], stop_reason='Maximum number of messages 3 reached, current message count: 3')
```

The conversation stopped after reaching the maximum message limit. Since the primary agent didn’t get to respond to the feedback, let’s continue the conversation.

```
# Use asyncio.run(...) if you are running this script as a standalone script.
await Console(round_robin_team.run_stream())
```

```
---------- primary ----------
Thank you for your feedback. Here is the revised Haiku:
Soft rain whispers down,
Cobblestones glisten softly —
Paris dreams in gray.
[Prompt tokens: 181, Completion tokens: 32]
---------- critic ----------
The revised Haiku now follows the traditional 5-7-5 syllable pattern, and it still beautifully captures the atmospheric mood of Paris in the rain. The imagery and flow are both clear and evocative. Well done on making the adjustment!
APPROVE
[Prompt tokens: 234, Completion tokens: 54]
---------- primary ----------
Thank you for your kind words and approval. I'm glad the revision meets your expectations and captures the essence of Paris. If you have any more requests or need further assistance, feel free to ask!
[Prompt tokens: 279, Completion tokens: 39]
---------- Summary ----------
Number of messages: 3
Finish reason: Maximum number of messages 3 reached, current message count: 3
Total prompt tokens: 694
Total completion tokens: 125
Duration: 6.43 seconds
```

```
TaskResult(messages=[TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=181, completion_tokens=32), content='Thank you for your feedback. Here is the revised Haiku:\n\nSoft rain whispers down, \nCobblestones glisten softly — \nParis dreams in gray.'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=234, completion_tokens=54), content='The revised Haiku now follows the traditional 5-7-5 syllable pattern, and it still beautifully captures the atmospheric mood of Paris in the rain. The imagery and flow are both clear and evocative. Well done on making the adjustment! \n\nAPPROVE'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=279, completion_tokens=39), content="Thank you for your kind words and approval. I'm glad the revision meets your expectations and captures the essence of Paris. If you have any more requests or need further assistance, feel free to ask!")], stop_reason='Maximum number of messages 3 reached, current message count: 3')
```

The team continued from where it left off, allowing the primary agent to respond to the feedback.

## Combining Termination Conditions[#](https://microsoft.github.io#combining-termination-conditions)

Let’s show how termination conditions can be combined using the AND (`&`

) and OR (`|`

) operators to create more complex termination logic. For example, we’ll create a team that stops either after 10 messages are generated or when the critic agent approves a message.

```
max_msg_termination = MaxMessageTermination(max_messages=10)
text_termination = TextMentionTermination("APPROVE")
combined_termination = max_msg_termination | text_termination
round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=combined_termination)
# Use asyncio.run(...) if you are running this script as a standalone script.
await Console(round_robin_team.run_stream(task="Write a unique, Haiku about the weather in Paris"))
```

```
---------- user ----------
Write a unique, Haiku about the weather in Paris
---------- primary ----------
Spring breeze gently hums,
Cherry blossoms in full bloom—
Paris wakes to life.
[Prompt tokens: 467, Completion tokens: 19]
---------- critic ----------
The Haiku beautifully captures the awakening of Paris in the spring. The imagery of a gentle spring breeze and cherry blossoms in full bloom effectively conveys the rejuvenating feel of the season. The final line, "Paris wakes to life," encapsulates the renewed energy and vibrancy of the city. The Haiku adheres to the 5-7-5 syllable structure and portrays a vivid seasonal transformation in a concise and poetic manner. Excellent work!
APPROVE
[Prompt tokens: 746, Completion tokens: 93]
---------- Summary ----------
Number of messages: 3
Finish reason: Text 'APPROVE' mentioned
Total prompt tokens: 1213
Total completion tokens: 112
Duration: 2.75 seconds
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, content='Write a unique, Haiku about the weather in Paris'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=467, completion_tokens=19), content='Spring breeze gently hums, \nCherry blossoms in full bloom— \nParis wakes to life.'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=746, completion_tokens=93), content='The Haiku beautifully captures the awakening of Paris in the spring. The imagery of a gentle spring breeze and cherry blossoms in full bloom effectively conveys the rejuvenating feel of the season. The final line, "Paris wakes to life," encapsulates the renewed energy and vibrancy of the city. The Haiku adheres to the 5-7-5 syllable structure and portrays a vivid seasonal transformation in a concise and poetic manner. Excellent work!\n\nAPPROVE')], stop_reason="Text 'APPROVE' mentioned")
```

The conversation stopped after the critic agent approved the message, although it could have also stopped if 10 messages were generated.

Alternatively, if we want to stop the run only when both conditions are met, we can use the AND (`&`

) operator.

```
combined_termination = max_msg_termination & text_termination
```

## Custom Termination Condition[#](https://microsoft.github.io#custom-termination-condition)

The built-in termination conditions are sufficient for most use cases.
However, there may be cases where you need to implement a custom termination condition that doesn’t fit into the existing ones.
You can do this by subclassing the [ TerminationCondition](https://microsoft.github.io/reference/python/autogen_agentchat.base.html#autogen_agentchat.base.TerminationCondition) class.

In this example, we create a custom termination condition that stops the conversation when a specific function call is made.

```
from typing import Sequence
from autogen_agentchat.base import TerminatedException, TerminationCondition
from autogen_agentchat.messages import AgentEvent, ChatMessage, StopMessage, ToolCallExecutionEvent
from autogen_core import Component
from pydantic import BaseModel
from typing_extensions import Self
class FunctionCallTerminationConfig(BaseModel):
"""Configuration for the termination condition to allow for serialization
and deserialization of the component.
"""
function_name: str
class FunctionCallTermination(TerminationCondition, Component[FunctionCallTerminationConfig]):
"""Terminate the conversation if a FunctionExecutionResult with a specific name is received."""
component_config_schema = FunctionCallTerminationConfig
"""The schema for the component configuration."""
def __init__(self, function_name: str) -> None:
self._terminated = False
self._function_name = function_name
@property
def terminated(self) -> bool:
return self._terminated
async def __call__(self, messages: Sequence[AgentEvent | ChatMessage]) -> StopMessage | None:
if self._terminated:
raise TerminatedException("Termination condition has already been reached")
for message in messages:
if isinstance(message, ToolCallExecutionEvent):
for execution in message.content:
if execution.name == self._function_name:
self._terminated = True
return StopMessage(
content=f"Function '{self._function_name}' was executed.",
source="FunctionCallTermination",
)
return None
async def reset(self) -> None:
self._terminated = False
def _to_config(self) -> FunctionCallTerminationConfig:
return FunctionCallTerminationConfig(
function_name=self._function_name,
)
@classmethod
def _from_config(cls, config: FunctionCallTerminationConfig) -> Self:
return cls(
function_name=config.function_name,
)
```

Let’s use this new termination condition to stop the conversation when the critic agent approves a message
using the `approve`

function call.

First we create a simple function that will be called when the critic agent approves a message.

```
def approve() -> None:
"""Approve the message when all feedbacks have been addressed."""
pass
```

Then we create the agents. The critic agent is equipped with the `approve`

tool.

```
from autogen_agentchat.agents import AssistantAgent
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.ui import Console
from autogen_ext.models.openai import OpenAIChatCompletionClient
model_client = OpenAIChatCompletionClient(
model="gpt-4o",
temperature=1,
# api_key="sk-...", # Optional if you have an OPENAI_API_KEY env variable set.
)
# Create the primary agent.
primary_agent = AssistantAgent(
"primary",
model_client=model_client,
system_message="You are a helpful AI assistant.",
)
# Create the critic agent with the approve function as a tool.
critic_agent = AssistantAgent(
"critic",
model_client=model_client,
tools=[approve], # Register the approve function as a tool.
system_message="Provide constructive feedback. Use the approve tool to approve when all feedbacks are addressed.",
)
```

Now, we create the termination condition and the team. We run the team with the poem-writing task.

```
function_call_termination = FunctionCallTermination(function_name="approve")
round_robin_team = RoundRobinGroupChat([primary_agent, critic_agent], termination_condition=function_call_termination)
# Use asyncio.run(...) if you are running this script as a standalone script.
await Console(round_robin_team.run_stream(task="Write a unique, Haiku about the weather in Paris"))
```

```
---------- user ----------
Write a unique, Haiku about the weather in Paris
---------- primary ----------
Raindrops gently fall,
Cobblestones shine in dim light—
Paris dreams in grey.
---------- critic ----------
This Haiku beautifully captures a melancholic yet romantic image of Paris in the rain. The use of sensory imagery like "Raindrops gently fall" and "Cobblestones shine" effectively paints a vivid picture. It could be interesting to experiment with more distinct seasonal elements of Paris, such as incorporating the Seine River or iconic landmarks in the context of the weather. Overall, it successfully conveys the atmosphere of Paris in subtle, poetic imagery.
---------- primary ----------
Thank you for your feedback! I’m glad you enjoyed the imagery. Here’s another Haiku that incorporates iconic Parisian elements:
Eiffel stands in mist,
Seine's ripple mirrors the sky—
Spring whispers anew.
---------- critic ----------
[FunctionCall(id='call_QEWJZ873EG4UIEpsQHi1HsAu', arguments='{}', name='approve')]
---------- critic ----------
[FunctionExecutionResult(content='None', name='approve', call_id='call_QEWJZ873EG4UIEpsQHi1HsAu', is_error=False)]
---------- critic ----------
None
```

```
TaskResult(messages=[TextMessage(source='user', models_usage=None, metadata={}, content='Write a unique, Haiku about the weather in Paris', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=30, completion_tokens=23), metadata={}, content='Raindrops gently fall, \nCobblestones shine in dim light— \nParis dreams in grey. ', type='TextMessage'), TextMessage(source='critic', models_usage=RequestUsage(prompt_tokens=99, completion_tokens=90), metadata={}, content='This Haiku beautifully captures a melancholic yet romantic image of Paris in the rain. The use of sensory imagery like "Raindrops gently fall" and "Cobblestones shine" effectively paints a vivid picture. It could be interesting to experiment with more distinct seasonal elements of Paris, such as incorporating the Seine River or iconic landmarks in the context of the weather. Overall, it successfully conveys the atmosphere of Paris in subtle, poetic imagery.', type='TextMessage'), TextMessage(source='primary', models_usage=RequestUsage(prompt_tokens=152, completion_tokens=48), metadata={}, content="Thank you for your feedback! I’m glad you enjoyed the imagery. Here’s another Haiku that incorporates iconic Parisian elements:\n\nEiffel stands in mist, \nSeine's ripple mirrors the sky— \nSpring whispers anew. ", type='TextMessage'), ToolCallRequestEvent(source='critic', models_usage=RequestUsage(prompt_tokens=246, completion_tokens=11), metadata={}, content=[FunctionCall(id='call_QEWJZ873EG4UIEpsQHi1HsAu', arguments='{}', name='approve')], type='ToolCallRequestEvent'), ToolCallExecutionEvent(source='critic', models_usage=None, metadata={}, content=[FunctionExecutionResult(content='None', name='approve', call_id='call_QEWJZ873EG4UIEpsQHi1HsAu', is_error=False)], type='ToolCallExecutionEvent'), ToolCallSummaryMessage(source='critic', models_usage=None, metadata={}, content='None', type='ToolCallSummaryMessage')], stop_reason="Function 'approve' was executed.")
```

You can see that the conversation stopped when the critic agent approved the message using the `approve`

function call.
</doc>

</project>